

# ä¸€æœºå™¨å­¦ä¹ 

[TOC]

## ç›‘ç£å­¦ä¹ 

- **ç›‘ç£å­¦ä¹ **ï¼Œç»™ç®—æ³•ä¸€ä¸ªæ•°æ®é›†ï¼Œæ•°æ®é›†ç”±â€˜æ­£ç¡®ç­”æ¡ˆâ€˜ç»„æˆï¼Œç„¶åæ ¹æ®å·²çŸ¥çš„æ­£ç¡®çš„æ ·æœ¬æ¥é¢„æµ‹æœªçŸ¥çš„æ•°æ®ç»“æœ
- â€™**å›å½’**ï¼šæ¨æµ‹ä¸€ç³»åˆ—è¿ç»­å€¼çš„ç»“æœ
- **åˆ†ç±»**ï¼šæ¨æµ‹å‡ºç¦»æ•£çš„è¾“å‡ºå€¼ï¼š0æˆ–1
  - å¯èƒ½ä¸æ­¢ä¸¤ä¸ªå€¼ï¼Œè€Œæ˜¯å¾ˆå¤šå’Œåˆ†ç±»

## æ— ç›‘ç£å­¦ä¹ 

- **æ— ç›‘ç£å­¦ä¹ **ï¼šåªæœ‰æ•°æ®é›†ä½†æ˜¯æœªçŸ¥æ ‡ç­¾å’Œç»“æœã€‚

äº¤ç»™ç®—æ³•å¤§é‡çš„æ•°æ®ï¼Œå¹¶è®©ç®—æ³•ä¸ºæˆ‘ä»¬ä»æ•°æ®ä¸­æ‰¾å‡ºæŸç§ç»“æ„

- **èšç±»ç®—æ³•**ï¼šæ— ç›‘ç£å­¦ä¹ å¯èƒ½ä¼šæŠŠæ•°æ®åˆ†æˆä¸åŒçš„ç°‡

- **é¸¡å°¾é…’é—®é¢˜å’Œç®—æ³•**ï¼šç»™å®šæ··åˆä¿¡å·ï¼Œå¦‚ä½•åˆ†ç¦»å‡ºå…¶ä¸­çš„ä¸åŒçš„ç‹¬ç«‹ä¿¡å·ã€‚



# äºŒã€å•å˜é‡çº¿æ€§å›å½’ï¼ˆLinear Regression with One Variableï¼‰

- çº¿æ€§å›å½’çš„æœ¬è´¨å°±æ˜¯æ‹Ÿåˆä¸€æ¡æ›²çº¿ï¼ˆå¤šæ¬¡é¡¹ä¹Ÿå¯ä»¥çœ‹æˆä¸€æ¬¡é¡¹é‚£ä¹ˆå°±æ˜¯ç›´çº¿ï¼‰hï¼ˆxï¼‰
- è¾“å‡ºå€¼å°±æ˜¯è¿ç»­çš„è¦å¾—åˆ°çš„å€¼
- ç”¨æ‹Ÿåˆå¥½çš„æ›²çº¿æ¥é¢„æµ‹

## 1.å•å˜é‡çº¿æ€§å›å½’æ¨¡å‹è¡¨ç¤º

- **ç›‘ç£å­¦ä¹ **ï¼šæ•°æ®é›†æ‹¥æœ‰æ­£ç¡®çš„ç­”æ¡ˆ
- **å›å½’é—®é¢˜**ï¼šæ ¹æ®ä¹‹å‰æ•°æ®é¢„æµ‹å‡ºçš„ä¸€ä¸ªå‡†ç¡®çš„è¾“å‡ºå€¼ã€‚
- **åˆ†ç±»**ï¼šæ¨æµ‹å‡ºç¦»æ•£çš„è¾“å‡ºå€¼ï¼š0æˆ–1
  - å¯èƒ½ä¸æ­¢ä¸¤ä¸ªå€¼ï¼Œè€Œæ˜¯å¾ˆå¤šå’Œåˆ†ç±»

- **å•å˜é‡çº¿æ€§å›å½’**

æˆ‘ä»¬å°†è¦ç”¨æ¥æè¿°è¿™ä¸ªå›å½’é—®é¢˜çš„æ ‡è®°å¦‚ä¸‹


- **ğ‘š** ä»£ è¡¨è®­ç»ƒé›†ä¸­å®ä¾‹çš„æ•°é‡
- **ğ‘¥** ä»£è¡¨ç‰¹å¾ /è¾“å…¥å˜é‡
- **ğ‘¦** ä»£è¡¨ç›®æ ‡å˜é‡ /è¾“å‡ºå˜é‡
- **(ğ‘¥,ğ‘¦)** ä»£è¡¨è®­ç»ƒé›†ä¸­çš„å®ä¾‹
- **(ğ‘¥(ğ‘–),ğ‘¦(ğ‘–))** ä»£è¡¨ç¬¬ **ğ‘–** ä¸ªè§‚å¯Ÿå®ä¾‹
- **â„** ä»£è¡¨å­¦ä¹ ç®—æ³•çš„è§£å†³æ–¹æ¡ˆæˆ–å‡½æ•°ä¹Ÿç§°ä¸ºå‡è®¾ï¼ˆ hypothesisï¼‰

![1610280700043](..\..\ç¬”è®°å›¾\å•å˜é‡æ˜¾ç¤ºå›å½’1)

ä¸€ç§å¯èƒ½çš„è¡¨è¾¾æ–¹å¼ä¸ºï¼ˆä¸€æ¬¡å‡½æ•°ï¼Œç›´çº¿ï¼‰ï¼š
**â„ğœƒ(ğ‘¥)=ğœƒ0+ğœƒ1ğ‘¥**ï¼Œå› ä¸ºåªå«æœ‰ä¸€ä¸ªç‰¹å¾ /è¾“å…¥å˜é‡ï¼Œå› æ­¤è¿™æ ·
çš„é—®é¢˜å«ä½œ



## 2.å‡è®¾å‡½æ•°hå’Œä»£ä»·å‡½æ•°J

- **â„ğœƒ(ğ‘¥)=ğœƒ~0~+ğœƒ~1~ğ‘¥** è¿™æ˜¯å‡è®¾å‡½æ•°
  - **ğœƒ<sub>0</sub>** å’Œ **ğœƒ<sub>1</sub>**è¡¨ç¤º**å‚æ•°parameters**ï¼Œåœ¨æˆ¿ä»·é—®é¢˜è¿™ä¸ªä¾‹å­ä¸­ä¾¿æ˜¯ç›´çº¿çš„æ–œç‡å’Œåœ¨yè½´ä¸Šçš„æˆªè·
  - **å‡è®¾å‡½æ•°æ˜¯å…³äºæ ·æœ¬ç‰¹å¾Xçš„å‡½æ•°**
-  $J \left( \theta_0, \theta_1 \right) = \frac{1}{2m}\sum\limits_{i=1}^m \left( h_{\theta}(x^{(i)})-y^{(i)} \right)^{2}$ **ä»£ä»·å‡½æ•°**
  - **ä»£ä»·å‡½æ•°æ˜¯å…³äºæ ·æœ¬å‚æ•°ğœƒçš„å‡½æ•°**
  - å»ºæ¨¡è¯¯å·®J ï¼ˆmodeling errorï¼‰**é¢„æµ‹å€¼å’Œè®­ç»ƒé›†æ±‡æ€»å®é™…å€¼å›ä¹‹é—´çš„å·®è·
  - ä½¿å¾—å»ºæ¨¡è¯¯å·®çš„å¹³æ–¹å’Œèƒ½å¤Ÿæœ€å°çš„å‡½æ•°ï¼Œè¿™é‡Œç”¨çš„æ˜¯**è¯¯å·®å¹³æ–¹å‡½æ•°**



## 3.ä»£ä»·å‡½æ•° cost functionç†è§£

- **å‡è®¾å‡½æ•°æ˜¯å…³äºæ ·æœ¬ç‰¹å¾Xçš„å‡½æ•°**
  - è¿™é‡Œæ˜¯ä¸ªä¸€æ¬¡å‡½æ•°ï¼Œè¡¨ç¤ºä¸€æ¡ç›´çº¿
- **ä»£ä»·å‡½æ•°J æ˜¯å…³äºæ ·æœ¬å‚æ•°ğœƒçš„å‡½æ•°**
  - æ¯ä¸ªğœƒéƒ½å¯¹åº”ä¸€ä¸ªh
  - æ˜¯ä¸ªäºŒæ¬¡å‡½æ•°ï¼Œæ¯ä¸ªğœƒéƒ½å¯¹åº”ä¸€ä¸ªä»£ä»·ï¼Œæˆ‘ä»¬è¦æ‰¾çš„å°±æ˜¯ä½¿Jæœ€å°çš„é‚£ä¸ªğœƒ

![](./images/10ba90df2ada721cf1850ab668204dc9.png)

- **ç®€å•æƒ…å†µï¼Œåªæœ‰ğœƒ~0~æ²¡æœ‰ğœƒ~1~**

![](./images/2c9fe871ca411ba557e65ac15d55745d.png)

- **æ›´å¤æ‚ä¸€ç‚¹ åŒæ—¶æ‹¥æœ‰ğœƒ~0~ğœƒ~1~**
  - **h**æ˜¯æœ‰æˆªè·çš„ç›´çº¿
  - **J**æ˜¯ä¸ªäºŒå…ƒå‡½æ•°ï¼Œç©ºé—´æ›²é¢
    - æŸ¥çœ‹ç­‰é«˜çº¿

![](./images/0b789788fc15889fe33fb44818c40852.png)

![](./images/86c827fe0978ebdd608505cd45feb774.png)

## 4.æ¢¯åº¦ä¸‹é™

### æ¢¯åº¦

- **æ–¹å‘å¯¼æ•°**

ä¸€å…ƒå‡½æ•°ä¸ºåˆ‡çº¿ï¼ŒäºŒå…ƒå‡½æ•°ä¸ºåˆ‡å¹³é¢ï¼Œé‚£ä¹ˆäºŒå…ƒå‡½æ•°åœ¨ä¸€ç‚¹è‡ªç„¶æœ‰æ— æ•°æ¡åˆ‡çº¿ï¼Œæ‰€ä»¥å¤šå…ƒå‡½æ•°çš„åˆ‡çº¿å’Œæ–¹å‘æœ‰å…³ã€‚

ä¸€ä¸ªäºŒå…ƒå‡½æ•°çš„æ–¹å‘å¯¼æ•°æ²¿ç€ **e= ï¼ˆcosÎ±ï¼ŒsinÎ±ï¼‰**æ–¹å‘çš„æ–¹å‘å¯¼æ•°ä¸º

![](../images/æ–¹å‘å¯¼æ•°.png)

- **å…³äºæ¢¯åº¦**

  - æ–¹å‘å¯¼æ•°æ²¿ä»»æ„æ–¹å‘æœ‰æ— æ•°ä¸ªå¯¼æ•°ï¼Œå“ªä¸ªæ–¹å‘çš„å¯¼æ•°å˜åŒ–ç‡æœ€å¿«å‘¢ï¼Œæ±‚å¾—ä¸Šå¼ä¸­çš„**Î±**ä½¿å¾—æ–¹å‘å¯¼æ•°æœ€å¤§å³å¯
    - å³å½“**e**è¿™ä¸ªæ–¹å‘å’Œ**g=f~x~ + f~y~** çš„æ–¹å‘ä¸€è‡´æ—¶ï¼Œå˜åŒ–ç‡æœ€å¤§ï¼Œè¿™ä¸ªgå°±æ˜¯æ¢¯åº¦ã€‚

  - æ¢¯åº¦æ–¹å‘æ˜¯å‡½æ•°å€¼å˜åŒ–æœ€å¿«çš„æ–¹å‘ï¼Œå˜åŒ–ç‡å¤§å°å°±æ˜¯æ¢¯åº¦çš„æ¨¡
    - ä¸ºä»€ä¹ˆéè¦æ±‚åå¯¼ï¼Œå°±æ˜¯å› ä¸ºè¿™æ˜¯å˜åŒ–æœ€å¿«çš„æ–¹å‘

![](../images/æ¢¯åº¦.png)



- **æ¢¯åº¦ä¸‹é™**

-  çŸ¥é“äº†æ¢¯åº¦çš„æ–¹å‘ï¼Œæ¯æ¬¡å°†å‚æ•°æ²¿ç€è¿™ä¸ªåæ–¹å‘ç§»åŠ¨ä¸€ç‚¹è·ç¦»ï¼Œåå¤è¿­ä»£
  - **åæ–¹å‘**å°±æ˜¯å‡ï¼Œä¸ç®¡è¿™ä¸€ç‚¹çš„æ­£è´Ÿ
  - ä¸€ç‚¹è·ç¦»å°±æ˜¯ä¹˜ä»¥**å­¦ä¹ ç‡**ï¼Œå› ä¸ºæ¢¯åº¦å¤§å°ï¼ˆå˜åŒ–ç‡å¤§å°ï¼‰ä¸ç­‰äºå‡½æ•°å®é™…ä¸Šä¼šå‡å°å¤šå°‘ã€‚æœ‰å¯èƒ½åœ¨è¿™ä¸€ç‚¹å˜åŒ–ç‡å¾ˆå¤§ä½†æ˜¯å‡½æ•°å‡å°æ…¢æ…¢å˜ç¼“ï¼Œæ‰€ä»¥å­¦ä¹ ç‡å¾ˆé‡è¦
  - **å¯¹äºä¸€å…ƒå‡½æ•°åªæœ‰æ­£iå’Œè´Ÿiæ‰€ä»¥å¯¼æ•°çš„å¤§å°å…¶å®ä¸é‡è¦ï¼Œä½ éšä¾¿ç»™ä¸€ä¸ªå€¼ä¸æ˜¯æ­£æ–¹å‘å°±æ˜¯åæ–¹å‘ï¼Œ**
    - **ä½†æ˜¯å¯¹äºå¤šå…ƒå‡½æ•°å°±ä¸ä¸€æ ·çš„äº†ï¼Œæ–¹å‘æœ‰æ— æ•°æ¡ï¼Œåªæœ‰åå¯¼çš„å¤§å°æ–¹å‘æ‰æ˜¯æœ€å¿«çš„æ–¹å‘ã€‚**





### æ¢¯åº¦ä¸‹é™

æ¢¯åº¦ä¸‹é™æ˜¯ä¸€ä¸ªç”¨æ¥æ±‚å‡½æ•°æœ€å°å€¼çš„ç®—æ³•ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨æ¢¯åº¦ä¸‹é™ç®—æ³•æ¥æ±‚å‡ºä»£ä»·å‡½æ•°$J(\theta_{0}, \theta_{1})$ çš„æœ€å°å€¼ã€‚

- å¼€å§‹æ—¶æˆ‘ä»¬éšæœºé€‰æ‹©ä¸€ä¸ªå‚æ•°çš„ç»„åˆ$\left( {\theta_{0}},{\theta_{1}},......,{\theta_{n}} \right)$ï¼Œè®¡ç®—ä»£ä»·å‡½æ•°ï¼Œç„¶åæˆ‘ä»¬å¯»æ‰¾ä¸‹ä¸€ä¸ªèƒ½è®©ä»£ä»·å‡½æ•°å€¼**ä¸‹é™æœ€å¤šçš„å‚æ•°ç»„åˆ**ã€‚
- æˆ‘ä»¬æŒç»­è¿™ä¹ˆåšç›´åˆ°æ‰¾åˆ°ä¸€ä¸ª**å±€éƒ¨æœ€å°å€¼**ï¼ˆ**local minimum**ï¼‰

æ¢¯åº¦ä¸‹é™æ³¨æ„ç‚¹æ˜¯ï¼š

- é€šå¸¸æœ€å¼€ä»0å¼€å§‹

- ä¸èƒ½ç¡®å®šæˆ‘ä»¬å¾—åˆ°çš„**å±€éƒ¨æœ€å°å€¼**æ˜¯å¦ä¾¿æ˜¯**å…¨å±€æœ€å°å€¼**ï¼ˆ**global minimum**ï¼‰ï¼Œ
- **é€‰æ‹©ä¸åŒçš„åˆå§‹å‚æ•°ç»„åˆï¼Œå¯èƒ½ä¼šæ‰¾åˆ°ä¸åŒçš„å±€éƒ¨æœ€å°å€¼ã€‚**

![](./images/db48c81304317847870d486ba5bb2015.jpg)





- **æ‰¹é‡æ¢¯åº¦ä¸‹é™**ï¼ˆ**batch gradient descent**ï¼‰ç®—æ³•çš„å…¬å¼ä¸ºï¼š



![](./images/ef4227864e3cabb9a3938386f857e938.png)

- å…¶ä¸­

  - $aâ€‹$æ˜¯**å­¦ä¹ ç‡**ï¼ˆ**learning rate**ï¼‰ï¼Œå®ƒå†³å®šäº†æˆ‘ä»¬æ²¿ç€èƒ½è®©ä»£ä»·å‡½æ•°ä¸‹é™ç¨‹åº¦æœ€å¤§çš„æ–¹å‘å‘ä¸‹**è¿ˆå‡ºçš„æ­¥å­**æœ‰å¤šå¤§ã€‚
    - Î±å¤ªå°ï¼Œä½ çš„è·Œæ‰“æ­¥æ•°ä¼šå¤ªå¤§
    - Î±å¤ªå¤§ï¼Œä½ çš„è¿­ä»£ä¸å®¹æ˜“æ”¶æ•›ï¼Œå®¹æ˜“å‘æ•£
    - æˆ‘ä»¬ä¸ç”¨æ”¹å˜é˜¿å°”æ³•çš„å€¼ï¼Œå› ä¸ºåœ¨è§£ç¦å±€éƒ¨æœ€å°å€¼æ—¶å€™ï¼Œå¯¼æ•°ä¼šé€æ¸è¶‹å‘äº0ï¼Œæ‰€ä»¥å–å‡ºçš„æ­¥ä¼ä¹Ÿä¼šè¶Šæ¥è¶Šå°ã€‚

  **åŒæ­¥æ›´æ–°** åœ¨æ‰¹é‡æ¢¯åº¦ä¸‹é™ä¸­ï¼Œæˆ‘ä»¬æ¯ä¸€æ¬¡éƒ½åŒæ—¶è®©æ‰€æœ‰çš„å‚æ•°å‡å»å­¦ä¹ é€Ÿç‡ä¹˜ä»¥ä»£ä»·å‡½æ•°çš„å¯¼æ•°ã€‚





### æ¢¯åº¦ä¸‹é™çš„çº¿æ€§å›å½’

å¯¹äºå•å˜é‡çš„çº¿æ€§å›å½’ç”¨æ¢¯åº¦ä¸‹é™ç®—æ³•ï¼Œå…³é”®åœ¨äºæ±‚å‡ºä»£ä»·å‡½æ•°çš„å¯¼æ•°

**$\frac{\partial }{\partial {{\theta }_{j}}}J({{\theta }_{0}},{{\theta }_{1}})=\frac{\partial }{\partial {{\theta }_{j}}}\frac{1}{2m}{{\sum\limits_{i=1}^{m}{\left( {{h}_{\theta }}({{x}^{(i)}})-{{y}^{(i)}} \right)}}^{2}}$**

$j=0$  æ—¶ï¼š$\frac{\partial }{\partial {{\theta }_{0}}}J({{\theta }_{0}},{{\theta }_{1}})=\frac{1}{m}{{\sum\limits_{i=1}^{m}{\left( {{h}_{\theta }}({{x}^{(i)}})-{{y}^{(i)}} \right)}}}â€‹$

$j=1$  æ—¶ï¼š$\frac{\partial }{\partial {{\theta }_{1}}}J({{\theta }_{0}},{{\theta }_{1}})=\frac{1}{m}\sum\limits_{i=1}^{m}{\left( \left( {{h}_{\theta }}({{x}^{(i)}})-{{y}^{(i)}} \right)\cdot {{x}^{(i)}} \right)}$

åˆ™ç®—æ³•æ”¹å†™æˆï¼šï¼ˆRepeatï¼‰æ˜¯è¿­ä»£çš„æ„æ€

**Repeat {**

â€‹                ${\theta_{0}}:={\theta_{0}}-a\frac{1}{m}\sum\limits_{i=1}^{m}{ \left({{h}_{\theta }}({{x}^{(i)}})-{{y}^{(i)}} \right)}$

â€‹                ${\theta_{1}}:={\theta_{1}}-a\frac{1}{m}\sum\limits_{i=1}^{m}{\left( \left({{h}_{\theta }}({{x}^{(i)}})-{{y}^{(i)}} \right)\cdot {{x}^{(i)}} \right)}$

â€‹               **}**

- **æ‰¹é‡æ¢¯åº¦ä¸‹é™**
  - æ¯ä¸€æ­¥æˆ‘ä»¬éƒ½ç”¨åˆ°äº†æ‰€æœ‰çš„è®­ç»ƒæ ·æœ¬





# ä¸‰ã€å¤šå˜é‡çº¿æ€§å›å½’

## 1.å¤šç»´ç‰¹å¾

- **å¤šç»´ç‰¹å¾**

  -  **n**ä»£è¡¨ç‰¹å¾çš„æ•°é‡

  - ${x^{\left( i \right)}}$ä»£è¡¨ç¬¬ $i$ ä¸ªè®­ç»ƒå®ä¾‹ï¼Œæ˜¯ç‰¹å¾çŸ©é˜µä¸­çš„ç¬¬$i$è¡Œï¼Œæ˜¯ä¸€ä¸ª**å‘é‡**ï¼ˆ**vector**ï¼‰ã€‚

    â€‹				æ¯”æ–¹è¯´ï¼Œä¸Šå›¾çš„

    â€‹					${x}^{(2)}\text{=}\begin{bmatrix} 1416\\\ 3\\\ 2\\\ 40 \end{bmatrix}$ï¼Œè¿™å…¶å®æ˜¯ä¸€ä¸ªè¡Œå‘é‡

    - ${x}_{j}^{\left( i \right)}$ä»£è¡¨ç‰¹å¾çŸ©é˜µä¸­ç¬¬ $i$ è¡Œçš„ç¬¬ $j$ ä¸ªç‰¹å¾ï¼Œä¹Ÿå°±æ˜¯ç¬¬ $i$ ä¸ªè®­ç»ƒå®ä¾‹çš„ç¬¬ $jâ€‹$ ä¸ªç‰¹å¾ã€‚

      â€‹			å¦‚ä¸Šå›¾çš„$x_{2}^{\left( 2 \right)}=3,x_{3}^{\left( 2 \right)}=2$ï¼Œ

      

- æ”¯æŒå¤šå˜é‡çš„**å‡è®¾ $h$** è¡¨ç¤ºä¸ºï¼š$h_{\theta}\left( x \right)={\theta_{0}}+{\theta_{1}}{x_{1}}+{\theta_{2}}{x_{2}}+...+{\theta_{n}}{x_{n}}$ï¼Œ

  - è¿™ä¸ªå…¬å¼ä¸­æœ‰**$n+1â€‹$ä¸ªå‚æ•°å’Œ$nâ€‹$ä¸ªå˜é‡**ï¼Œä¸ºäº†ä½¿å¾—å…¬å¼èƒ½å¤Ÿç®€åŒ–ä¸€äº›ï¼Œå¼•å…¥$x_{0}=1â€‹$ï¼Œåˆ™å…¬å¼è½¬åŒ–ä¸ºï¼š$h_{\theta} \left( x \right)={\theta_{0}}{x_{0}}+{\theta_{1}}{x_{1}}+{\theta_{2}}{x_{2}}+...+{\theta_{n}}{x_{n}}â€‹$

æ­¤æ—¶æ¨¡å‹ä¸­çš„å‚æ•°æ˜¯ä¸€ä¸ª$n+1$ç»´çš„å‘é‡ï¼Œä»»ä½•ä¸€ä¸ªè®­ç»ƒå®ä¾‹ä¹Ÿéƒ½æ˜¯$n+1$ç»´çš„å‘é‡ï¼Œ



## 2.çŸ©é˜µå½¢å¼çš„å‡è®¾å‡½æ•°

- **ç‰¹å¾çŸ©é˜µ$X$çš„ç»´åº¦æ˜¯  $m*(n+1)$**

 å› æ­¤å…¬å¼å¯ä»¥ç®€åŒ–ä¸ºï¼š

- $h_{\theta} \left( x \right)={\theta^{T}}X$ï¼Œå…¶ä¸­ä¸Šæ ‡$T$ä»£è¡¨çŸ©é˜µè½¬ç½®ã€‚ï¼ˆå› ä¸ºè¿™é‡Œéƒ½æ˜¯åˆ—å‘é‡ï¼Œæ‰€ä»¥è¦è½¬ç½®ï¼‰
  - Xå¯ä»¥æ˜¯mè¡ŒNåˆ—ä¹Ÿå¯ä»¥æ˜¯nè¡Œmåˆ—ï¼Œç›¸åº”çš„å…¬å¼ä¼šå·¦ä¹˜æˆ–è€…å³ä¹˜ï¼ŒXç‰¹å¾å’Œyå±•å¼€çš„æ–¹å‘æ˜¯ä¸€æ ·çš„
  - æ‰€æœ‰çš„ä¸€ç»´å‘é‡éƒ½æ˜¯åˆ—å‘é‡ï¼Œå“ªæ€•ä½ å–å‡ºè¡Œæ¥ä¹Ÿæ˜¯åˆ—å‘é‡ï¼ŒæŠŠåˆ—å‘é‡å±•å¼€å°±ä»å˜æˆè¡Œç„¶åå±•å¼€
    - ï¼ˆmï¼Œnï¼‰ dot ï¼ˆnï¼Œ1ï¼‰ = ï¼ˆmï¼Œ1ï¼‰
    - ï¼ˆ1ï¼Œnï¼‰dotï¼ˆnï¼Œmï¼‰=ï¼ˆ1ï¼Œnï¼‰
    - æŠŠmä¸ªæ ·æœ¬çœ‹åšä¸€ä¸ªæ ·æœ¬Xn

![](./images/å¤šå˜é‡å‡è®¾å‡½æ•°.png)



## 3.å¤šå˜é‡æ¢¯åº¦ä¸‹é™

å’Œå•å˜é‡ç›¸ä¼¼ï¼Œæˆ‘ä»¬ä¹Ÿæ„å»ºä¸€ä¸ªä»£ä»·å‡½æ•°

$J\left( {\theta_{0}},{\theta_{1}}...{\theta_{n}} \right)=\frac{1}{2m}\sum\limits_{i=1}^{m}{{{\left( h_{\theta} \left({x}^{\left( i \right)} \right)-{y}^{\left( i \right)} \right)}^{2}}}$ ï¼Œ

å…¶ä¸­ï¼š$h_{\theta}\left( x \right)=\theta^{T}X={\theta_{0}}+{\theta_{1}}{x_{1}}+{\theta_{2}}{x_{2}}+...+{\theta_{n}}{x_{n}}$ ï¼Œ

æˆ‘ä»¬çš„ç›®æ ‡å’Œå•å˜é‡çº¿æ€§å›å½’é—®é¢˜ä¸­ä¸€æ ·ï¼Œæ˜¯è¦æ‰¾å‡ºä½¿å¾—ä»£ä»·å‡½æ•°æœ€å°çš„ä¸€ç³»åˆ—å‚æ•°ã€‚

![](./images/41797ceb7293b838a3125ba945624cf6.png)

æ±‚å¯¼åå¾—åˆ°

![](./images/dd33179ceccbd8b0b59a5ae698847049.png)

å½“$n>=1$æ—¶ï¼Œ
${{\theta }_{0}}:={{\theta }_{0}}-a\frac{1}{m}\sum\limits_{i=1}^{m}{({{h}_{\theta }}({{x}^{(i)}})-{{y}^{(i)}})}x_{0}^{(i)}$

${{\theta }_{1}}:={{\theta }_{1}}-a\frac{1}{m}\sum\limits_{i=1}^{m}{({{h}_{\theta }}({{x}^{(i)}})-{{y}^{(i)}})}x_{1}^{(i)}$

${{\theta }_{2}}:={{\theta }_{2}}-a\frac{1}{m}\sum\limits_{i=1}^{m}{({{h}_{\theta }}({{x}^{(i)}})-{{y}^{(i)}})}x_{2}^{(i)}$

æˆ‘ä»¬å¼€å§‹éšæœºé€‰æ‹©ä¸€ç³»åˆ—çš„å‚æ•°å€¼ï¼Œè®¡ç®—æ‰€æœ‰çš„é¢„æµ‹ç»“æœåï¼Œå†ç»™æ‰€æœ‰çš„å‚æ•°ä¸€ä¸ªæ–°çš„å€¼ï¼Œå¦‚æ­¤å¾ªç¯ç›´åˆ°æ”¶æ•›ã€‚

**Python** ä»£ç ï¼š

ä»£ä»·å‡½æ•°çš„å®ç°

```python
def computeCost(X, y, theta):
    # numpyçš„æ•°ç»„ éƒ½æ˜¯è¡Œä¸ºä¸€ç»´çš„ï¼Œmä¸ªæ ·æœ¬nä¸ªç‰¹å¾æ•°é‡æ‰€ä»¥æ˜¯X * theta.Tï¼Œpoweræ˜¯æ•°ç»„å…ƒç´ æ±‚næ¬¡æ–¹
    inner = np.power(((X * theta.T) - y), 2)
    return np.sum(inner) / (2 * len(X)) # é•¿åº¦æ˜¯m
```

æ¢¯åº¦ä¸‹é™çš„å®ç°

```python
def gradientDescent(X, y, theta, alpha, iters):
    temp = np.matrix(np.zeros(theta.shape))
    parameters = int(theta.ravel().shape[1])#å±•å¼€
    cost = np.zeros(iters)
    
    for i in range(iters):
        error = (X * theta.T) - y
        
        for j in range(parameters):
            term = np.multiply(error, X[:,j])
            temp[0,j] = theta[0,j] - ((alpha / len(X)) * np.sum(term))
            
        theta = temp
        cost[i] = computeCost(X, y, theta)
        
    return theta, cost
```





### 3.1.æ¢¯åº¦ä¸‹é™æ³•å®è·µ1-ç‰¹å¾ç¼©æ”¾ï¼ˆfeature scalingï¼‰â€”â€”ç‰¹å¾å½’ä¸€åŒ– 

åœ¨æˆ‘ä»¬é¢å¯¹å¤šç»´ç‰¹å¾é—®é¢˜çš„æ—¶å€™ï¼Œæˆ‘ä»¬è¦ä¿è¯è¿™äº›ç‰¹å¾éƒ½å…·æœ‰ç›¸è¿‘çš„å°ºåº¦ï¼Œè¿™å°†å¸®åŠ©æ¢¯åº¦ä¸‹é™ç®—æ³•æ›´å¿«åœ°æ”¶æ•›ã€‚

ä»¥æˆ¿ä»·é—®é¢˜ä¸ºä¾‹ï¼Œå°ºå¯¸çš„å€¼ä¸º 0-2000å¹³æ–¹è‹±å°ºï¼Œè€Œæˆ¿é—´æ•°é‡çš„å€¼åˆ™æ˜¯0-5ï¼Œæ•°é‡çº§ç›¸å·®å¤ªå¤§ï¼Œçœ‹å‡ºå›¾åƒä¼šæ˜¾å¾—å¾ˆæ‰ï¼Œæ¢¯åº¦ä¸‹é™ç®—æ³•éœ€è¦éå¸¸å¤šæ¬¡çš„è¿­ä»£æ‰èƒ½æ”¶æ•›ã€‚

è§£å†³çš„æ–¹æ³•

- æ˜¯å°è¯•å°†æ‰€æœ‰**ç‰¹å¾çš„å°ºåº¦**éƒ½å°½é‡ç¼©æ”¾åˆ°**-1åˆ°1**ä¹‹é—´ã€‚å¦‚å›¾ï¼š
  - è¿™é‡Œæ˜¯å°†X1 X2ç‰¹å¾å€¼ç¼©æ”¾åˆ°ä¸€ä¸ªç›¸åŒçš„å°ºåº¦å†…
  - åŒæ—¶å¯ä»¥åšmean normalization å³ç‰¹å¾å€¼çš„å¹³å‡å€¼å˜æˆ0
    - **X  å˜æˆ ï¼ˆX -meanï¼ˆXï¼‰ï¼‰/S **   Sæ˜¯èŒƒå›´

![](./images/b8167ff0926046e112acf789dba98057.png)

pythonä»£ç   ç”¨å‡å€¼ç‰¹å¾å½’ä¸€åŒ–

```python
data2 = (data2 - data2.mean()) / data2.std()
```



### 3.2æ¢¯åº¦ä¸‹é™æ³•å®è·µ2â€”å­¦ä¹ ç‡learning rate Î±

æ¢¯åº¦ä¸‹é™ç®—æ³•çš„æ¯æ¬¡è¿­ä»£å—åˆ°å­¦ä¹ ç‡çš„å½±å“ï¼Œ

- å¦‚æœå­¦ä¹ ç‡$a$è¿‡å°ï¼Œåˆ™è¾¾åˆ°æ”¶æ•›æ‰€éœ€çš„è¿­ä»£æ¬¡æ•°ä¼šéå¸¸é«˜ï¼›

- å¦‚æœå­¦ä¹ ç‡$a$è¿‡å¤§ï¼Œæ¯æ¬¡è¿­ä»£å¯èƒ½ä¸ä¼šå‡å°ä»£ä»·å‡½æ•°ï¼Œå¯èƒ½ä¼šè¶Šè¿‡å±€éƒ¨æœ€å°å€¼å¯¼è‡´æ— æ³•æ”¶æ•›

é€šå¸¸å¯ä»¥è€ƒè™‘å°è¯•äº›å­¦ä¹ ç‡ï¼š

$\alpha=0.01ï¼Œ0.03ï¼Œ0.1ï¼Œ0.3ï¼Œ1ï¼Œ3ï¼Œ10$

è¿­ä»£æ¬¡æ•°å’Œä»£ä»·å‡½æ•°çš„æ›²çº¿

![](./images/cd4e3df45c34f6a8e2bb7cd3a2849e6c.jpg)

ä¹Ÿæœ‰ä¸€äº›è‡ªåŠ¨æµ‹è¯•æ˜¯å¦æ”¶æ•›çš„æ–¹æ³•ï¼Œä¾‹å¦‚å°†ä»£ä»·å‡½æ•°çš„å˜åŒ–å€¼ä¸æŸä¸ªé˜€å€¼ï¼ˆä¾‹å¦‚0.001ï¼‰è¿›è¡Œæ¯”è¾ƒï¼Œä½†é€šå¸¸çœ‹ä¸Šé¢è¿™æ ·çš„å›¾è¡¨æ›´å¥½ã€‚

## 4ç‰¹å¾çš„é€‰æ‹©ï¼Œå’Œå¤šé¡¹å¼å›å½’

ä¾‹å¦‚ä¸€ä¸ªæˆ¿ä»·é—®é¢˜ï¼Œç»™å‡ºçš„ç‰¹å¾æ˜¯æˆ¿å­çš„é•¿fronttageå’Œå®½depthï¼Œæˆ‘ä»¬å¯èƒ½ä¼šç”¨è¿™ä¸¤ä¸ªç‰¹å¾æ¥ç»™å‡ºå‡è®¾å‡½æ•°

â€‹					$h_{\theta}\left( x \right)={\theta_{0}}+{\theta_{1}}\times{frontage}+{\theta_{2}}\times{depth}$ 

ä½†æ˜¯å…¶å®é•¿å’Œå®½å¹¶éç‹¬ç«‹æ€§çš„ï¼ŒçœŸæ­£çš„ç‰¹å¾åº”å½“æ˜¯é•¿ä¹˜ä»¥å®½çš„é¢ç§¯æ‰€ä»¥

â€‹					$x=frontage*depth=areaâ€‹$

åˆ™å‡è®¾å‡½æ•°åº”å½“ä¸º

â€‹					${h_{\theta}}\left( x \right)={\theta_{0}}+{\theta_{1}}x$ã€‚

- **å¤šé¡¹å¼å›å½’**

- çº¿æ€§å›å½’å¹¶ä¸é€‚åˆæ‰€æœ‰çš„æ•°æ®ï¼Œæœ‰æ—¶å€™æˆ‘ä»¬éœ€è¦**å¤šæ¬¡æ–¹çš„å¤šé¡¹å¼**
  - ä¸€ä¸ªäºŒæ¬¡æ–¹æ¨¡å‹ï¼š$h_{\theta}\left( x \right)={\theta_{0}}+{\theta_{1}}{x_{1}}+{\theta_{2}}{x_{2}^2}â€‹$
  - ä¸‰æ¬¡æ–¹æ¨¡å‹ï¼š $h_{\theta}\left( x \right)={\theta_{0}}+{\theta_{1}}{x_{1}}+{\theta_{2}}{x_{2}^2}+{\theta_{3}}{x_{3}^3}$ 

![](./images/3a47e15258012b06b34d4e05fb3af2cf.jpg)

- **å¤šé¡¹å¼å›å½’çš„æœ¬è´¨è¿˜æ˜¯çº¿æ€§å›å½’ã€‚è¦æ”¹å˜çš„åªæ˜¯ä½ çš„ç‰¹å¾å€¼è€Œå·²**

  - æˆ‘ä»¬å¯ä»¥ä»¤ï¼š

    ${{x}_{2}}=x_{2}^{2},{{x}_{3}}=x_{3}^{3}$ï¼Œä»è€Œå°†æ¨¡å‹è½¬åŒ–ä¸ºçº¿æ€§å›å½’æ¨¡å‹ã€‚

  - æ ¹æ®å‡½æ•°å›¾å½¢ç‰¹æ€§ï¼Œæˆ‘ä»¬è¿˜å¯ä»¥ä½¿ï¼š

    ${{{h}}_{\theta}}(x)={{\theta }_{0}}\text{+}{{\theta }_{1}}(size)+{{\theta}_{2}}{{(size)}^{2}}$

    æˆ–è€…:

    ${{{h}}_{\theta}}(x)={{\theta }_{0}}\text{+}{{\theta }_{1}}(size)+{{\theta }_{2}}\sqrt{size}$

- **å¤šé¡¹å¼å›å½’å¿…é¡»æ³¨æ„ç‰¹å¾å€¼çš„å½’ä¸€åŒ–ï¼Œ**å¦‚æœæˆ‘ä»¬é‡‡ç”¨å¤šé¡¹å¼å›å½’æ¨¡å‹ï¼Œåœ¨è¿è¡Œæ¢¯åº¦ä¸‹é™ç®—æ³•å‰ï¼Œç‰¹å¾ç¼©æ”¾éå¸¸æœ‰å¿…è¦ã€‚

  - æ¯”å¦‚Xçš„èŒƒå›´æ˜¯1~1000ï¼Œä½ ä½¿ç”¨xçš„3æ¬¡æ–¹åèŒƒå›´å°±å˜æˆ 1â€”1000000000





# å››ï¼Œæ­£è§„æ–¹ç¨‹ç®—æ³•å¤„ç†çº¿æ€§å›å½’

## 1.æ­£è§„æ–¹ç¨‹

åˆ°ç›®å‰ä¸ºæ­¢ï¼Œæˆ‘ä»¬éƒ½åœ¨ä½¿ç”¨æ¢¯åº¦ä¸‹é™ç®—æ³•ï¼Œä½†æ˜¯å¯¹äºæŸäº›çº¿æ€§å›å½’é—®é¢˜ï¼Œæ­£è§„æ–¹ç¨‹æ–¹æ³•æ˜¯æ›´å¥½çš„è§£å†³æ–¹æ¡ˆã€‚å¦‚ï¼š

![](./images/a47ec797d8a9c331e02ed90bca48a24b.png)



æ­£è§„æ–¹ç¨‹æ˜¯é€šè¿‡æ±‚è§£ä¸‹é¢çš„æ–¹ç¨‹æ¥æ‰¾å‡ºä½¿å¾—ä»£ä»·å‡½æ•°æœ€å°çš„å‚æ•°çš„ï¼š$\frac{\partial}{\partial{\theta_{j}}}J\left( {\theta_{j}} \right)=0$ 

å‡è®¾æˆ‘ä»¬çš„è®­ç»ƒé›†ç‰¹å¾çŸ©é˜µä¸º $X$ï¼ˆåŒ…å«äº† ${{x}_{0}}=1$ï¼‰å¹¶ä¸”æˆ‘ä»¬çš„è®­ç»ƒé›†ç»“æœä¸ºå‘é‡ $y$ï¼Œåˆ™åˆ©ç”¨æ­£è§„æ–¹ç¨‹è§£å‡ºå‘é‡

- â€‹                                $\theta ={{\left( {X^T}X \right)}^{-1}}{X^{T}}yâ€‹$ ã€‚
  - ä¸Šæ ‡**T**ä»£è¡¨çŸ©é˜µè½¬ç½®ï¼Œä¸Šæ ‡-1 ä»£è¡¨çŸ©é˜µçš„é€†ã€‚è®¾çŸ©é˜µ$A={X^{T}}X$ï¼Œåˆ™ï¼š

${{\left( {X^T}X \right)}^{-1}}={A^{-1}}â€‹$

**å¦‚æœæˆ‘ä»¬çš„XçŸ©é˜µååˆ†ç®€å•çš„è¯ï¼Œå¯ä»¥ç›´æ¥å¸¦å…¥ä¸Šå¼ç›´æ¥æ±‚å‡º$\thetaâ€‹$**

- **æ³¨æ„è¿™é‡Œçš„XçŸ©é˜µ**æ¯ä¸ªæ ·æœ¬æ˜¯ä½œä¸ºè¡Œå‘é‡å­˜åœ¨çš„ï¼Œé‚£ä¹ˆXçŸ©é˜µæ˜¯ä¸ª1Xmçš„åˆ—å‘é‡

- **æ³¨**ï¼š**å¯¹äºé‚£äº›ä¸å¯é€†çš„çŸ©é˜µ**ï¼ˆé€šå¸¸æ˜¯å› ä¸ºç‰¹å¾ä¹‹é—´ä¸ç‹¬ç«‹ï¼Œå¦‚åŒæ—¶åŒ…å«è‹±å°ºä¸ºå•ä½çš„å°ºå¯¸å’Œç±³ä¸ºå•ä½çš„å°ºå¯¸ä¸¤ä¸ªç‰¹å¾ï¼Œä¹Ÿæœ‰å¯èƒ½æ˜¯ç‰¹å¾æ•°é‡å¤§äºè®­ç»ƒé›†çš„æ•°é‡ï¼‰ï¼Œ**æ­£è§„æ–¹ç¨‹æ–¹æ³•æ˜¯ä¸èƒ½ç”¨çš„**ã€‚
  - å› ä¸ºå¯é€†çŸ©é˜µä¸€å®šæ˜¯æ–¹é˜µï¼Œåˆ—æˆ–è¡Œçº¿æ€§ç›¸å…³ä¸€å®šä¸å¯é€†



ä¸¾ä¸ªä¾‹å­![](./images/261a11d6bce6690121f26ee369b9e9d1.png)

![](../images/c8eedc42ed9feb21fac64e4de8d39a06.png)

![](../images/b62d24a1f709496a6d7c65f87464e911.jpg)

æ¢¯åº¦ä¸‹é™ä¸æ­£è§„æ–¹ç¨‹çš„æ¯”è¾ƒï¼š

| æ¢¯åº¦ä¸‹é™                      | æ­£è§„æ–¹ç¨‹                                                     |
| ----------------------------- | ------------------------------------------------------------ |
| éœ€è¦é€‰æ‹©å­¦ä¹ ç‡$\alpha$        | ä¸éœ€è¦                                                       |
| éœ€è¦å¤šæ¬¡è¿­ä»£                  | ä¸€æ¬¡è¿ç®—å¾—å‡º                                                 |
| å½“ç‰¹å¾æ•°é‡$n$å¤§æ—¶ä¹Ÿèƒ½è¾ƒå¥½é€‚ç”¨ | éœ€è¦è®¡ç®—${{\left( {{X}^{T}}X \right)}^{-1}}$ å¦‚æœç‰¹å¾æ•°é‡nè¾ƒå¤§åˆ™è¿ç®—ä»£ä»·å¤§ï¼Œå› ä¸ºçŸ©é˜µé€†çš„è®¡ç®—æ—¶é—´å¤æ‚åº¦ä¸º$O\left( {{n}^{3}} \right)$ï¼Œé€šå¸¸æ¥è¯´å½“$n$å°äº10000 æ—¶è¿˜æ˜¯å¯ä»¥æ¥å—çš„ |
| é€‚ç”¨äºå„ç§ç±»å‹çš„æ¨¡å‹          | åªé€‚ç”¨äºçº¿æ€§æ¨¡å‹ï¼Œä¸é€‚åˆé€»è¾‘å›å½’æ¨¡å‹ç­‰å…¶ä»–æ¨¡å‹               |



æ­£è§„æ–¹ç¨‹çš„**python**å®ç°ï¼š

```python
import numpy as np
    
 def normalEqn(X, y):
    
   theta = np.linalg.inv(X.T@X)@X.T@y 
#np.linalg.inv æ„æ€ä¸ºçŸ©é˜µæ±‚é€†
#X.T@Xç­‰ä»·äºX.T.dot(X)
    
   return theta
```







### 1.1æ­£è§„æ–¹ç¨‹æ±‚å¯¼è¯¦ç»†è¿‡ç¨‹

https://zhuanlan.zhihu.com/p/60719445è¯¦ç»†å¯¹ä»£ä»·å‡½æ•°æ±‚å¯¼çš„è¿‡ç¨‹,æ­¥éª¤å¦‚ä¸‹ï¼š

**å¢åŠ å†…å®¹ï¼š**

$\theta ={{\left( {X^{T}}X \right)}^{-1}}{X^{T}}y$ çš„æ¨å¯¼è¿‡ç¨‹ï¼š

$J\left( \theta  \right)=\frac{1}{2m}\sum\limits_{i=1}^{m}{{{\left( {h_{\theta}}\left( {x^{(i)}} \right)-{y^{(i)}} \right)}^{2}}}$
å…¶ä¸­ï¼š${h_{\theta}}\left( x \right)={\theta^{T}}X={\theta_{0}}{x_{0}}+{\theta_{1}}{x_{1}}+{\theta_{2}}{x_{2}}+...+{\theta_{n}}{x_{n}}$ Xæ˜¯ç‰¹å¾çŸ©é˜µ

å°†å‘é‡è¡¨è¾¾å½¢å¼è½¬ä¸ºçŸ©é˜µè¡¨è¾¾å½¢å¼ï¼Œåˆ™æœ‰$J(\theta )=\frac{1}{2}{{\left( X\theta -y\right)}^{2}}â€‹$ ï¼Œå…¶ä¸­$Xâ€‹$ä¸º$mâ€‹$è¡Œ$nâ€‹$åˆ—çš„çŸ©é˜µï¼ˆ$mâ€‹$ä¸ºæ ·æœ¬ä¸ªæ•°ï¼Œ$nâ€‹$ä¸ºç‰¹å¾ä¸ªæ•°ï¼‰ï¼Œ$\thetaâ€‹$ä¸º$nâ€‹$è¡Œ1åˆ—çš„çŸ©é˜µï¼Œ$yâ€‹$ä¸º$mâ€‹$è¡Œ1åˆ—çš„çŸ©é˜µï¼Œå¯¹$J(\theta )â€‹$è¿›è¡Œå¦‚ä¸‹å˜æ¢

- **è¿™é‡ŒXæ˜¯ ä¸ªmè¡Œnåˆ—çš„çŸ©é˜µï¼Œåœ¨æœ€å¼€å§‹çš„å…¬å¼ä¸­æ—¶ä¸€ä¸ªnè¡Œmåˆ—çš„çŸ©é˜µ**

- **ï¼ˆXÎ¸-Yï¼‰æ˜¯ä¸€ä¸ªä¸€ç»´å‘é‡ï¼Œä¸€ç»´å‘é‡å’Œä»–çš„è½¬ç½®ç›¸ä¹˜å°±æ˜¯å¹³æ–¹çš„ä¸€ç»´å‘é‡**

$J(\theta )=\frac{1}{2}{{\left( X\theta -y\right)}^{T}}\left( X\theta -y \right)â€‹$

â€‹     $=\frac{1}{2}\left( {{\theta }^{T}}{{X}^{T}}-{{y}^{T}} \right)\left(X\theta -y \right)$

â€‹     $=\frac{1}{2}\left( {{\theta }^{T}}{{X}^{T}}X\theta -{{\theta}^{T}}{{X}^{T}}y-{{y}^{T}}X\theta -{{y}^{T}}y \right)$

æ¥ä¸‹æ¥å¯¹$J(\theta )â€‹$åå¯¼ï¼Œéœ€è¦ç”¨åˆ°ä»¥ä¸‹å‡ ä¸ªçŸ©é˜µçš„æ±‚å¯¼æ³•åˆ™:

$\frac{dAB}{dB}={{A}^{T}}$ 

$\frac{d{{X}^{T}}AX}{dX}=2AX$                            

æ‰€ä»¥æœ‰:

$\frac{\partial J\left( \theta  \right)}{\partial \theta }=\frac{1}{2}\left(2{{X}^{T}}X\theta -{{X}^{T}}y -{}({{y}^{T}}X )^{T}-0 \right)â€‹$

$=\frac{1}{2}\left(2{{X}^{T}}X\theta -{{X}^{T}}y -{{X}^{T}}y -0 \right)$

â€‹           $={{X}^{T}}X\theta -{{X}^{T}}y$

ä»¤$\frac{\partial J\left( \theta  \right)}{\partial \theta }=0$,

åˆ™æœ‰$\theta ={{\left( {X^{T}}X \right)}^{-1}}{X^{T}}y$



# äº”ï¼Œsklearn å’Œpython

## 1ç”¨pythonå¤„ç†çº¿æ€§å›å½’

```python
# æ¢¯åº¦ä¸‹é™
def gradientDescent(X, y, theta, alpha, iters):
    temp = np.matrix(np.zeros(theta.shape))
    parameters = int(theta.ravel().shape[1])
    cost = np.zeros(iters)
    
    for i in range(iters):
        error = (X * theta.T) - y
        
        for j in range(parameters):
            term = np.multiply(error, X[:,j])
            temp[0,j] = theta[0,j] - ((alpha / len(X)) * np.sum(term))
            
        theta = temp
        cost[i] = computeCost(X, y, theta)
        
    return theta, cost
#è®¡ç®—æŸå¤±å€¼
def computeCost(X, y, theta):
    inner = np.power(((X * theta.T) - y), 2)
    return np.sum(inner) / (2 * len(X))

path =  'ex1data2.txt'
data2 = pd.read_csv(path, header=None, names=['Size', 'Bedrooms', 'Price'])
alpha = 0.01
iters = 1000
# æ·»åŠ ä¸€åˆ—1
data2.insert(0, 'Ones', 1)

# X è®­ç»ƒé›†å’Œyè®­ç»ƒé›†
cols = data2.shape[1]
X2 = data2.iloc[:,0:cols-1]
y2 = data2.iloc[:,cols-1:cols]

# 
X2 = np.matrix(X2.values)
y2 = np.matrix(y2.values)
theta2 = np.matrix(np.array([0,0,0]))

# æ¢¯åº¦ä¸‹é™
g2, cost2 = gradientDescent(X2, y2, theta2, alpha, iters)

# è¿­ä»£åç®—
cost_last = computeCost(X2, y2, g2)
# ç”»å›¾
fig, ax = plt.subplots(figsize=(12,8))
ax.plot(np.arange(iters), cost2, 'r')
ax.set_xlabel('Iterations')
ax.set_ylabel('Cost')
ax.set_title('Error vs. Training Epoch')
plt.show()
```

## 2 ç”¨sklernå¤„ç†çº¿æ€§å›å½’

- å®šä¹‰ä¸€ä¸ªçº¿æ€§å›å½’å¯¹è±¡**lr = sklearn.linear_model.LinearRegression(fit_intercept=True, normalize=False, copy_X=True, n_jobs=1)**
  - **fit_intercept :** é»˜è®¤ä¸ºTrue,æ˜¯å¦è®¡ç®—è¯¥æ¨¡å‹çš„æˆªè·ã€‚å¦‚æœä½¿ç”¨ä¸­å¿ƒåŒ–çš„æ•°æ®ï¼Œå¯ä»¥è€ƒè™‘è®¾ç½®ä¸ºFalse,ä¸è€ƒè™‘æˆªè·ã€‚æ³¨æ„è¿™é‡Œæ˜¯è€ƒè™‘ï¼Œä¸€èˆ¬è¿˜æ˜¯è¦è€ƒè™‘æˆªè·
  - **normalize:** é»˜è®¤ä¸ºfalse. å½“fit_interceptè®¾ç½®ä¸ºfalseçš„æ—¶å€™ï¼Œè¿™ä¸ªå‚æ•°ä¼šè¢«è‡ªåŠ¨å¿½ç•¥ã€‚å¦‚æœä¸ºTrue,å›å½’å™¨ä¼šæ ‡å‡†åŒ–è¾“å…¥å‚æ•°ï¼šå‡å»å¹³å‡å€¼ï¼Œå¹¶ä¸”é™¤ä»¥ç›¸åº”çš„äºŒèŒƒæ•°ã€‚å½“ç„¶å•¦ï¼Œåœ¨è¿™é‡Œè¿˜æ˜¯å»ºè®®å°†æ ‡å‡†åŒ–çš„å·¥ä½œæ”¾åœ¨è®­ç»ƒæ¨¡å‹ä¹‹å‰ã€‚é€šè¿‡è®¾ç½®**sklearn.preprocessing.StandardScaler**æ¥å®ç°ï¼Œè€Œåœ¨æ­¤å¤„è®¾ç½®ä¸ºfalse
  - **copy_X :** é»˜è®¤ä¸ºTrue, å¦åˆ™Xä¼šè¢«æ”¹å†™
  - **n_jobs:** int é»˜è®¤ä¸º1. å½“-1æ—¶é»˜è®¤ä½¿ç”¨å…¨éƒ¨CPUs ??(è¿™ä¸ªå‚æ•°æœ‰å¾…å°è¯•)
- ç”¨è¯¥å¯¹è±¡è®­æ•°æ®**lr.fit(X,y,sample_weight=None):**
  - **X**: array, ç¨€ç–çŸ©é˜µ [n_samples,n_features]
  - **y**: array [n_samples, n_targets]
  - **sample_weight**: æƒé‡ array [n_samples]

çº¿æ€§å›å½’å¯¹è±¡å¯è°ƒç”¨çš„**å±æ€§**

- **lr.coef**_ï¼šè®­ç»ƒåçš„è¾“å…¥ç«¯æ¨¡å‹ç³»æ•°ï¼Œå¦‚æœlabelæœ‰ä¸¤ä¸ªï¼Œå³yå€¼æœ‰ä¸¤åˆ—ã€‚é‚£ä¹ˆæ˜¯ä¸€ä¸ª2Dçš„array
- **lr.intercept_**ï¼šæˆªè·

çº¿æ€§å›å½’å¯¹è±¡å¯è°ƒç”¨çš„**æ–¹æ³•**

- ç”¨è¯¥å¯¹è±¡è®­æ•°æ®**lr.fit(X,y,sample_weight=None):**
  - **X**: array, ç¨€ç–çŸ©é˜µ [n_samples,n_features]
  - **y**: array [n_samples, n_targets]
  - **sample_weight**: æƒé‡ array [n_samples]

- **get_params(deep=True)ï¼š** è¿”å›å¯¹regressor çš„è®¾ç½®å€¼

- **lr.predict(x_test)**ï¼šé¢„æµ‹æ•°æ®
- **lr.score**(x_test,y_test)ï¼šè¯„ä¼°,æœ€å¥½æ˜¯1.0
  - å®šä¹‰ä¸º(1-u/v)ï¼Œå…¶ä¸­u = ï¼ˆï¼ˆy_true - y_predï¼‰^2ï¼‰.sum()ï¼Œè€Œv=((y_true-y_true.mean())**2).mean()

```python
from sklearn import linear_model

model = linear_model.LinearRegression()
model.fit(X, y)# ç”¨è®­ç»ƒé›†è®­ç»ƒ

x = np.array(X[:, 1].A1)
f = model.predict(X).flatten() # ç”¨è®­ç»ƒå¥½çš„æ¨¡å‹é¢„æµ‹

fig, ax = plt.subplots(figsize=(12,8))
ax.plot(x, f, 'r', label='Prediction')
ax.scatter(data.Population, data.Profit, label='Traning Data')
ax.legend(loc=2)
ax.set_xlabel('Population')
ax.set_ylabel('Profit')
ax.set_title('Predicted Profit vs. Population Size')
plt.show()
```

- **sklearn.linear_model.Ridge:**å²­å›å½’ å°±æ˜¯l2æ­£åˆ™åŒ–
- **sklearn.linear_model.Lasso:**ç½—æ£®å›å½’å°±æ˜¯l1æ­£åˆ™åŒ–



## 3æ­£è§„æ–¹ç¨‹çš„å®ç°

- $\theta ={{\left( {X^{T}}X \right)}^{-1}}{X^{T}}yâ€‹$

```python
# æ­£è§„æ–¹ç¨‹
def normalEqn(X, y):
    theta = np.linalg.inv(X.T@X)@X.T@y
    # invæ˜¯æ±‚çŸ©é˜µçš„é€†
    #X.T@Xç­‰ä»·äºX.T.dot(X)
    return theta
```

