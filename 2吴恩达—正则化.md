# 正则化（Regularization)

[TOC]



- **范数**
  - L0范数： 所有非零元素的个数
  - L1范数：所有非零元素的和
  - L2范数：所有元素的平方和



正则化的意义就是，在代价函数中添加一个范数项。以L2范数为例，增加了所有元素的平方和再乘以惩罚系数，最小化代价函数就要考虑正则化项

- 惩罚系数小时，正则化项趋于0，基本无效果
- 惩罚系数大时，正则化项影响代价函数，我们就会选择小的参树趋向于0

**几何解释**

![](F:/AI10%E5%9F%BA%E7%A1%8010%E6%9C%9F%E4%B8%8A%E8%AF%BE%E7%AC%94%E8%AE%B0%E5%92%8C%E9%A1%B9%E7%9B%AE/%E4%B8%8A%E8%AF%BE%E7%AC%94%E8%AE%B0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/images/%E6%AD%A3%E5%88%99%E5%8C%96.png)

w为参数，L2正则化项把W限定在了一个圆的范围内

是对 w 的平方和做数值上界限定，即所有w 的平方和不超过参数 C。这时候，我们的目标就转换为：最小化训练样本误差 Ein，但是要遵循 w 平方和小于 C 的条件。

于是本来梯度下降是按蓝色方向下降，但是现在受到圆的影响只能沿着圆切线方向绿色方向下降，最终在离最小点最近的圆上一点停止

![](F:/AI10%E5%9F%BA%E7%A1%8010%E6%9C%9F%E4%B8%8A%E8%AF%BE%E7%AC%94%E8%AE%B0%E5%92%8C%E9%A1%B9%E7%9B%AE/%E4%B8%8A%E8%AF%BE%E7%AC%94%E8%AE%B0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/images/%E6%AD%A3%E5%88%99%E5%8C%96L2.png)

其他的范数



![](F:/AI10%E5%9F%BA%E7%A1%8010%E6%9C%9F%E4%B8%8A%E8%AF%BE%E7%AC%94%E8%AE%B0%E5%92%8C%E9%A1%B9%E7%9B%AE/%E4%B8%8A%E8%AF%BE%E7%AC%94%E8%AE%B0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/images/%E8%8C%83%E6%95%B0.png)

**因为L1范数在坐标轴上取得交点，所以更容易得到稀疏矩阵**

## 1.过拟合问题

在拟合的时候会产生**过拟合问题（over-fitting）**

**正则化（Regularization）**可以改善或减少过拟合问题

下图是一个回归问题的例子：

![](F:/AI10%E5%9F%BA%E7%A1%8010%E6%9C%9F%E4%B8%8A%E8%AF%BE%E7%AC%94%E8%AE%B0%E5%92%8C%E9%A1%B9%E7%9B%AE/%E4%B8%8A%E8%AF%BE%E7%AC%94%E8%AE%B0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/images/72f84165fbf1753cd516e65d5e91c0d3.jpg)

![](F:/AI10%E5%9F%BA%E7%A1%8010%E6%9C%9F%E4%B8%8A%E8%AF%BE%E7%AC%94%E8%AE%B0%E5%92%8C%E9%A1%B9%E7%9B%AE/%E4%B8%8A%E8%AF%BE%E7%AC%94%E8%AE%B0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/images/be39b497588499d671942cc15026e4a2.jpg)

- 第一个模型**欠拟合**，**高偏差bias**，模型不准确
- 第二个模型适合
- 第三个模型**过拟合**，高方差**Variance(方差)** 模型不稳定



- **多项式回归**

就以多项式理解，$x$ 的次数越高，拟合的越好，但相应的预测的能力就可能变差。

问题是，如果我们发现了过拟合问题，应该如何处理？

1. 丢弃一些不能帮助我们正确预测的特征。可以是手工选择保留哪些特征，或者使用一些模型选择的算法来帮忙（例如**PCA**）
2. 正则化。 保留所有的特征，但是减少参数的大小（**magnitude**）。



## 2.正则化——代价函数

${h_\theta}\left( x \right)={\theta_{0}}+{\theta_{1}}{x_{1}}+{\theta_{2}}{x_{2}^2}+{\theta_{3}}{x_{3}^3}+{\theta_{4}}{x_{4}^4}$

过拟合的一个直接原因就是我们的模型过于复杂时，高次项太多,所以一个思路就是如何让高次项的系数变小接近于0

我们决定要减少${\theta_{3}}$和${\theta_{4}}$的大小，我们要做的便是修改代价函数，在其中${\theta_{3}}$和${\theta_{4}}$ 设置一点惩罚。这样做的话，我们在尝试最小化代价时也需要将这个惩罚纳入考虑中，并最终导致选择较小一些的${\theta_{3}}$和${\theta_{4}}$。

修改后的代价函数如下：$\underset{\theta }{\mathop{\min }}\,\frac{1}{2m}[\sum\limits_{i=1}^{m}{{{\left( {{h}_{\theta }}\left( {{x}^{(i)}} \right)-{{y}^{(i)}} \right)}^{2}}+1000\theta _{3}^{2}+10000\theta _{4}^{2}]}$

这样我们就相当于最终会选择一个较小的${\theta_{3}}$和${\theta_{4}}$ 

于是，更一般的形式是

- $J\left( \theta  \right)=\frac{1}{2m}[\sum\limits_{i=1}^{m}{{{({h_\theta}({{x}^{(i)}})-{{y}^{(i)}})}^{2}}+\lambda \sum\limits_{j=1}^{n}{\theta_{j}^{2}}]}$
  - $\lambda $又称为正则化参数（**Regularization Parameter**）
    - $\lambda $过大时，惩罚太重，所有$\theta$都接近于0，拟合曲线就变成直线了
    - $\lambda $过小，惩罚不够，接会过拟合
  - 这个平方和其实就是**L2范数**
  - 根据惯例，我们不对${\theta_{0}}$ 进行惩罚

![](F:/AI10%E5%9F%BA%E7%A1%8010%E6%9C%9F%E4%B8%8A%E8%AF%BE%E7%AC%94%E8%AE%B0%E5%92%8C%E9%A1%B9%E7%9B%AE/%E4%B8%8A%E8%AF%BE%E7%AC%94%E8%AE%B0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/images/ea76cc5394cf298f2414f230bcded0bd.jpg)

那为什么增加的一项$\lambda =\sum\limits_{j=1}^{n}{\theta_j^{2}}$ 可以使$\theta $的值减小呢？
因为如果我们令 $\lambda$ 的值很大的话，为了使**Cost Function** 尽可能的小，所有的 $\theta $ 的值（不包括${\theta_{0}}$）都会在一定程度上减小。
但若$\lambda$ 的值太大了，那么$\theta $（不包括${\theta_{0}}$）都会趋近于0，这样我们所得到的只能是一条平行于$x$轴的直线。
所以对于正则化，我们要取一个合理的 $\lambda$ 的值，这样才能更好的应用正则化。



## 3.L2范数正则化线性回归

正则化线性回归的代价函数为：正则化项为L2范数

$J\left( \theta  \right)=\frac{1}{2m}\sum\limits_{i=1}^{m}{[({{({h_\theta}({{x}^{(i)}})-{{y}^{(i)}})}^{2}}+\lambda \sum\limits_{j=1}^{n}{\theta _{j}^{2}})]}$

- **梯度下降法正则化代价函数**

如果我们要使用梯度下降法令这个代价函数最小化，因为我们未对$\theta_0$进行正则化，所以梯度下降算法将**分两种情形**：

$Repeat$  $until$  $convergence${

​                                                   ${\theta_0}:={\theta_0}-a\frac{1}{m}\sum\limits_{i=1}^{m}{(({h_\theta}({{x}^{(i)}})-{{y}^{(i)}})x_{0}^{(i)}})$ 

​                                                   ${\theta_j}:={\theta_j}-a[\frac{1}{m}\sum\limits_{i=1}^{m}{(({h_\theta}({{x}^{(i)}})-{{y}^{(i)}})x_{j}^{\left( i \right)}}+\frac{\lambda }{m}{\theta_j}]$ 

​                                                             $for$ $j=1,2,...n$

​                                                   }

对上面的算法中$ j=1,2,...,n$ 时的更新式子进行调整可得：

​			${\theta_j}:={\theta_j}(1-a\frac{\lambda }{m})-a\frac{1}{m}\sum\limits_{i=1}^{m}{({h_\theta}({{x}^{(i)}})-{{y}^{(i)}})x_{j}^{\left( i \right)}}$ 

可以看出，正则化线性回归的梯度下降算法的变化在于，**每次都在原有算法更新规则的基础上令$\theta $值减少了一个额外的值$a\frac{\lambda }{m}$。这一项就是惩罚项，每次都让$\theta$减小一点。**



- **正规方程正则化**

不加正则化时正规方程求解

​                    $\theta ={{\left( {X^T}X \right)}^{-1}}{X^{T}}y$ 

正规方程加上正则化项L2其偏导等于0的结果为

![](F:/AI10%E5%9F%BA%E7%A1%8010%E6%9C%9F%E4%B8%8A%E8%AF%BE%E7%AC%94%E8%AE%B0%E5%92%8C%E9%A1%B9%E7%9B%AE/%E4%B8%8A%E8%AF%BE%E7%AC%94%E8%AE%B0/images/71d723ddb5863c943fcd4e6951114ee3.png)

可以看到多了一个正则化参数乘以对角阵。

矩阵尺寸为 $(n+1)*(n+1)$。因为X多了一项常数项

**值得注意的是**：如果X 为（m，n）形状，特征很多，n远>m个数时候，X是不可逆的，X^T^X同样不可逆，是不能用正规方程的

****但是正则化后的括号中矩阵是一个可逆的矩阵，这也是正则化后的好处**



## 4.L2范数正则化逻辑回归

回顾逻辑回归的代价函数

​				$J\left( \theta  \right)=\frac{1}{m}\sum\limits_{i=1}^{m}{{Cost}\left( {h_\theta}\left( {x}^{\left( i \right)} \right),{y}^{\left( i \right)} \right)}$，

其中

![](F:/AI10%E5%9F%BA%E7%A1%8010%E6%9C%9F%E4%B8%8A%E8%AF%BE%E7%AC%94%E8%AE%B0%E5%92%8C%E9%A1%B9%E7%9B%AE/%E4%B8%8A%E8%AF%BE%E7%AC%94%E8%AE%B0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/images/54249cb51f0086fa6a805291bf2639f1.png)



$Cost\left( {h_\theta}\left( x \right),y \right)=-y\times log\left( {h_\theta}\left( x \right) \right)-(1-y)\times log\left( 1-{h_\theta}\left( x \right) \right)$
带入代价函数得到：
$J\left( \theta  \right)=\frac{1}{m}\sum\limits_{i=1}^{m}{[-{{y}^{(i)}}\log \left( {h_\theta}\left( {{x}^{(i)}} \right) \right)-\left( 1-{{y}^{(i)}} \right)\log \left( 1-{h_\theta}\left( {{x}^{(i)}} \right) \right)]}$

- **带入正则化选项后的逻辑回归代价函数**

$J\left( \theta  \right)=\frac{1}{m}\sum\limits_{i=1}^{m}{[-{{y}^{(i)}}\log \left( {h_\theta}\left( {{x}^{(i)}} \right) \right)-\left( 1-{{y}^{(i)}} \right)\log \left( 1-{h_\theta}\left( {{x}^{(i)}} \right) \right)]}$

​					$+\frac{\lambda }{2m}\sum\limits_{j=1}^{n}{\theta _{j}^{2}}$

同样，由于$\theta $只从1开始到n进行惩罚，不惩罚第0项

**求偏导然后梯度下降**

$Repeat$  $until$  $convergence${

​                                                   ${\theta_0}:={\theta_0}-a\frac{1}{m}\sum\limits_{i=1}^{m}{(({h_\theta}({{x}^{(i)}})-{{y}^{(i)}})x_{0}^{(i)}})$

​                                                  ${\theta_j}:={\theta_j}-a[\frac{1}{m}\sum\limits_{i=1}^{m}{({h_\theta}({{x}^{(i)}})-{{y}^{(i)}})x_{j}^{\left( i \right)}}+\frac{\lambda }{m}{\theta_j}]$

​                                                 $for$ $j=1,2,...n$

​                                                 }

**注：**看上去同线性回归一样，但是知道 ${h_\theta}\left( x \right)=g\left( {\theta^T}X \right)$，所以与线性回归不同。

```python
import numpy as np

def costReg(theta, X, y, learningRate):
    theta = np.matrix(theta)
    X = np.matrix(X)
    y = np.matrix(y)
    first = np.multiply(-y, np.log(sigmoid(X*theta.T)))
    second = np.multiply((1 - y), np.log(1 - sigmoid(X*theta.T)))
    reg = (learningRate / (2 * len(X))* np.sum(np.power(theta[:,1:theta.shape[1]],2))
    return np.sum(first - second) / (len(X)) + reg
```

## 5.python代码实现

**python实现正则化**

```python
import numpy as np
import pandas as pd
import scipy.optimize as opt

path = 'python代码/ex2-logistic regression/ex2data2.txt'
data2 = pd.read_csv(path, header=None, names=['Test 1', 'Test 2', 'Accepted'])
print(data2.head())

degree = 5
x1 = data2['Test 1']
x2 = data2['Test 2']

data2.insert(3, 'Ones', 1)
# 这一步是构建多项式
for i in range(1, degree):
    for j in range(0, i):
        data2['F' + str(i) + str(j)] = np.power(x1, i-j) * np.power(x2, j)

data2.drop('Test 1', axis=1, inplace=True)
data2.drop('Test 2', axis=1, inplace=True)

print(data2.head())


# sigmoid函数
def sigmoid(z):
    return 1 / (1 + np.exp(-z))


# 正则化代价函数
def cost(theta, X, y, learningRate=1):
    first = np.multiply(-y, np.log(sigmoid(X * theta.T)))
    second = np.multiply((1 - y), np.log(1 - sigmoid(X * theta.T)))
    reg1 = learningRate / (2 * len(X))
    reg = reg1 * np.sum(np.power(theta[1:theta.shape[0]], 2))
    return np.sum(first - second) / len(X) + reg


# 梯度下降
def regularized_cost(theta, X, y, l=1):
    #     '''you don't penalize theta_0'''
    theta_j1_to_n = theta[1:]
    regularized_term = (l / (2 * len(X))) * np.power(theta_j1_to_n, 2).sum()

    return cost(theta, X, y) + regularized_term


def gradient(theta, X, y):
    parameters = int(theta.shape[0])  # 特征的数量
    grad = np.zeros(parameters)

    error = sigmoid(X * theta.T) - y

    for i in range(parameters):  # 每个特征都要算，因为公式里有个Xj
        x1 = X[:, i].reshape(118, 1)

        term = np.multiply(error, x1)
        grad[i] = np.sum(term) / len(X)

    return grad


def regularized_gradient(theta, X, y, l=1):
#     '''still, leave theta_0 alone'''
    theta_j1_to_n = theta[1:]
    regularized_theta = (l / len(X)) * theta_j1_to_n

    # by doing this, no offset is on theta_0
    regularized_term = np.concatenate([np.array([0]), regularized_theta])

    return gradient(theta, X, y) + regularized_term



# set X and y (remember from above that we moved the label to column 0)
cols = data2.shape[1]
X2 = data2.iloc[:,1:cols]
y2 = data2.iloc[:,0:1]

# convert to numpy arrays and initalize the parameter array theta
X2 = np.array(X2.values)
y2 = np.array(y2.values)
theta2 = np.zeros(11)
# 学习率
learningRate = 1
# 初始损失
print(cost(theta2, X2, y2, learningRate))
# 梯度下降

result2 = opt.fmin_tnc(func=cost, x0=theta2, fprime=regularized_gradient, args=(X2, y2))
theta_min = result2[0]

def predict(theta, X):
    probability = sigmoid(np.dot(X, theta.T))
    list1 = []
    for i in probability:
        if i >= 0.5:
            list1.append(1)
        else:list1.append(0)
    return list1


predictions = predict(theta_min, X2)
correct = [1 if ((a == 1 and b == 1) or (a == 0 and b == 0)) else 0 for (a, b) in zip(predictions, y2)]
accuracy = (sum(map(int, correct)) % len(correct))
print ('accuracy = {0}%'.format(accuracy))
```



- **sklearn实现正则化**

```python
from sklearn import linear_model#调用sklearn的线性回归包
import numpy as np
import pandas as pd


path = 'python代码/ex2-logistic regression/ex2data2.txt'
data2 = pd.read_csv(path, header=None, names=['Test 1', 'Test 2', 'Accepted'])
print(data2.head())
degree = 5
x1 = data2['Test 1']
x2 = data2['Test 2']

data2.insert(3, 'Ones', 1)
# 这一步是构建多项式
for i in range(1, degree):
    for j in range(0, i):
        data2['F' + str(i) + str(j)] = np.power(x1, i-j) * np.power(x2, j)

data2.drop('Test 1', axis=1, inplace=True)
data2.drop('Test 2', axis=1, inplace=True)

print(data2.head())
cols = data2.shape[1]
X2 = data2.iloc[:,1:cols]
y2 = np.array(data2.iloc[:,0:1]).flatten()


model = linear_model.LogisticRegression(penalty='l2', C=1.0)
model.fit(X2, y2)
print(model.score(X2[10:30], y2[10:30]))
```

