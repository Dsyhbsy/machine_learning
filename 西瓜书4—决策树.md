# 决策树

## 1.基本流程

- **决策树**

**决策树（decision tree）**是一种模仿人类决策的学习方法。举个例子，比方说买电脑，我们首先看看外观帅不帅气，然后再看看性能怎么样，还得看看价格如何，最终**经过一系列的判断做出**是否购买电脑的**决策**。

一棵决策树可以分成三个部分：叶节点，非叶节点，分支。**叶节点**对应**决策结果**，也即分类任务中的类别标记；**非叶节点**（包括根节点）对应一个**判定问题**（某属性=？）；**分支**对应父节点**判定问题的不同答案**（可能的属性值），可能连向一个非叶节点的子节点，也可能连向叶节点。

决策就是从根节点开始走到叶节点的过程。每经过一个节点的判定，数据集就按照答案（属性值）划分为若干子集，**在子节点做判定时只需要考虑对应的数据子集就可以了**。

决策树学习的目的是为了**产生一棵泛化能力强，即处理未见示例能力强的决策树**。



- **决策树是一个递归过程**

生成算法：

1. 传入训练集和属性集
2. 生成一个新节点
3. 若此时数据集中所有样本都属于同一类，则把新节点设置为该类的叶节点，然后**返回**$^1$。
4. 若此时属性集为空，或者数据集中所有样本在属性集余下的所有属性上取值都相同，无法进一步划分，则把新节点设置为叶节点，类标记为数据集中样本数最多的类，然后**返回**$^2$
5. 从属性集中选择一个最优划分属性
   - 为该属性的每个属性值生成一个分支，并按属性值划分出子数据集
   - 若分支对应的子数据集为空，无法进一步划分，则直接把子节点设置为叶节点，类标记为父节点数据集中样本数最多的类，然后**返回**$^3$
   - 将子数据集和去掉了划分属性的子属性集作为算法的传入参数，继续生成该分支的子决策树。

稍微注意以下，3处返回中的第2处和第3处设置叶节点的类标记原理有所不同。第2处将类标记设置为当前节点对应为数据集中样本数最多的类，这是利用当前节点的**后验分布**；第3处将类标记设置为为父节点数据集中样本数最多的类，这是把父节点的样本分布作为当前节点的**先验分布**。

## 2.  划分选择—信息熵

决策树中，我们判定的初衷是希望划分后需要考虑的可能性更少，或者说希望子节点的纯度更高，混轮程度更低，（确定性越高）

- **信息熵**

是一种衡量样本集纯度的常用指标：熵就是混乱程度，不确定性

熵越大，混乱程度越高，不确定性越高

熵越小，混轮程度越低，确定性越高，更能确定是哪一类

$$Ent(D) = -\sum_{k=1}^{|\mathcal{Y}|}p_klog_2p_k$$

**一定要记得最前面的负号！！！**其中 $|\mathcal{Y}|$ 为类别集合，$p_k$ 为该类样本占样本总数的比例。



- **信息增益，ID3算法**

$$Gain(D,a) = Ent(D) - \sum_{v=1}^{V}\frac{|D^v|}{|D|}Ent(D^v)$$

前半项是信息熵

后一项表示用a这个属性划分能产生V个节点，那么这个划分Dv的信息熵的权重为多少

整个公式表示**信息增益越大，代表用属性 $a$ 进行划分所获得的纯度提升越大**。

其中 $V$ 表示属性 $a$ 的属性值集合，

$D^v$ 表示属性值为 $v$ 的数据子集。

求和项也称为**条件熵**，

我们可以理解为**它是先求出每个数据子集的信息熵，然后按每个数据子集占原数据集的比例来赋予权重，比例越大，对提升纯度的帮助就越大。**



- **互信息**信息增益又称为**互信息（Mutual information）**。
  - 一个连续变量X的不确定性，用方差Var(X)来度量
  - 一个离散变量X的不确定性，用熵H(X)来度量
  - 两个连续变量X和Y的相关度，用协方差或相关系数来度量
  - 两个离散变量X和Y的相关度，用互信息I(X;Y)来度量(直观地，X和Y的相关度越高，X对分类的作用就越大)



- **信息熵例子**

![](./images/信息熵1.png)

![](./images/信息熵2.png)

![](./images/信息熵3.png)



## 3. 其他划分—增益率，基尼系数

事实上在上面的例子中，西瓜的编号也是属性，也应该计算它的信息熵，结果为0.998，远大于其他属性，说明编号是个很确定的分类方法（因为每个编号只有一个样本，非常的确定不混乱）。

所以用信息增益不具有泛化能力，对那些能分出很多分支的属性有所偏好（因为他们信息增益大，熵降低的多）

- **增益率 C4,5算法**

其中，

$$IV(a) = -\sum_{v=1}^V\frac{|D^v|}{|D|}log_2\frac{|D^v|}{|D|}$$

**一定要记得最前面的负号！！！**IV称为属性的**固有值（intrinsic value）**

**属性a的取值树木越多，V就越大，衡量的是样本集在某个属性上的混乱程度**

增益率准则对可取值**数目较少的属性有所偏好**因此， C4.5
算法并不是直接选择增益率最大的候选划分属性，而是使用了一个启发式，先从候选划分属性中找出信息增益高于平均水平的属性，再从中选择增益率最高的.



- **基尼系数，CART算法**

基尼值：

$$Gini(D) = \sum_{k=1}^{|\mathcal{Y}|}\sum_{k' \neq k}p_kp_{k'}\\
=1-\sum_{k=1}^{|\mathcal{Y}|}p_k^2$$

基尼指数：

$$Gini\_index(D,a) = \sum_{v=1}^{V}\frac{|D^v|}{|D|}Gini(D^v)$$

反映的是**从一个数据集中随机抽取两个样本，其类别标志不同的概率**。

**基尼值越小，样本集的纯度越高**。

于是，我们在候选属性集合A 中，选择那个使得划分后基尼指数最小的属性作为最优划分属性



## 4. 剪枝处理

为了防止过拟合，分支太多的话不利于模型泛化，需要去掉一些分支

- **预剪枝**

往下分支之前，先把这个节点当做叶节点不划分，用验证集来计算验证精度， 划分之后再计算精度，如果划分后的精度高于不划分，则可以划分，否则就不划分

![](./images/预剪枝.png)

- **后剪枝**

先生成完整的决策树，然后从底部开始逐个考察节点，如果不展开节点的精度比展开后的精度高，则删去这个节点下面的分支。

若精度相同，也会剪枝保证精简模型

- **优缺点**

预剪枝是一种贪心策略，降低了过拟合风险，同时也**显著减少了模型的训练时间开销和测试时间开销**。但是！这种贪心策略有可能导致**欠拟合**

后剪枝是种比较保守的策略，**欠拟合的风险很小，泛化性能往往优于预剪枝的决策树**。但是由于后剪枝是在生成了完整决策树后，自底向上对所有非叶节点进行考察，所以**训练时间开销要比未剪枝决策树和预剪枝决策树都大得多**。



## 5连续值和缺失值

- **连续值属性**

有时候会碰到属性是连续而非离散的情况。我们用**二分法**来离散化这个连续属性

给定一个包含连续属性 $a$ 的数据集，并且 $a$ 在数据集中有 $n$ 个不同取值，我们先把属性 $a$ 的 $n$ 个属性值**从小到大进行排序**。**所谓“二分”是指将这些属性值分为两个类别**（比方说把身高这一属性分为高于170和低于170两个类别）。

在对连续属性值排序完之后，由于有 $n$ 个不同取值，取每**两个取值的平均值作为划分点**的话，就有 $n-1$ 个候选划分点。我们需要做得就是按照准则（比方说用ID3算法的话就是信息增益）进行 $n-1$ 次判断。每次拿出一个候选划分点，把连续属性分为两类，转换为离散属性。然后基于这个基础计算准则，**最终选出一个最优的属性值划分点。**

注意！和离散属性不同，连续属性用于当前节点的划分后，其**后代节点依然可以使用该连续属性进一步划分**。比方说当前节点用身高低于170划分了，那么它的后代节点还可以用身高低于160来进一步划分。



- **缺失值**

假设数据集为 $D$，有缺失值的属性为 $a$，令 $\tilde{D}$ 表示 $D$ 中没有缺失属性 $a$ 的样本子集。我们只能基于 $\tilde{D}$ 来判断属性 $a$ 的优劣。但是我们又希望包含缺失值的样本也能在建模过程体现出一定的影响了，因此要**重新定义准则**。在那之前，先定义几个新定义用到的变量：

$$\rho = \frac{\sum_{\mathbf{x} \in \tilde{D}}w_\mathbf{x}}{\sum_{\mathbf{x} \in D}w_\mathbf{x}}$$

$$\tilde{p_k} = \frac{\sum_{\mathbf{x} \in \tilde{D_k}}w_\mathbf{x}}{\sum_{\mathbf{x} \in \tilde{D}}w_\mathbf{x}},\quad (1 \leq k \leq |\mathcal{Y}|)$$

$$\tilde{r_v} = \frac{\sum_{\mathbf{x} \in \tilde{D^v}}w_\mathbf{x}}{\sum_{\mathbf{x} \in \tilde{D}}w_\mathbf{x}},\quad (1 \leq v \leq V)$$

$\rho$ 表示**无缺失值样本所占的比例**;

$\tilde{p_k}$ 表示**无缺失值样本中第 $k$ 类所占的比例**;

$\tilde{r_v}$ 表示**无缺失值样本中在属性 $a$ 上取值 $a^v$ 的样本所占的比例** ;

注意，这里的 $w_\mathbf{x}$ 表示样本的权值，它是**含缺失值样本参与建模**的一种方式。在根节点处初始时，所有样本 $\mathbf{x}$ 的权重都为1。

接下来重新定义信息熵和信息增益，推广到样本含缺失值的情况：

$$Ent(\tilde{D}) = -\sum_{k=1}^{|\mathcal{Y|}}\tilde{p_k}log_2\tilde{p_k}$$

$$Gain(D,a) = \rho \times Gain(\tilde{D},a)\\
= \rho \times (Ent(\tilde{D}) - \sum_{v=1}^V\tilde{r_v}Ent(\tilde{D^v}))$$

按照新的定义来计算包含缺失值的属性的信息增益，然后和其他属性的信息增益相比，选出最优的。

- **给定划分属性，如何划分缺失该属性值的样本？**

假设有一个包含缺失值的属性被计算出是最优划分属性，那么我们就要按该属性的不同取值划分数据集了。缺失该属性值的样本怎么划分呢？答案是**按概率划分**，这样的样本会被**同时划入所有子节点**，并且其**权重更新**为对应的 $\tilde{r_v} \dot w_\mathbf{x}$。

可以把无缺失值的决策树建模想象为各样本权值恒为1的情形，它们**只对自己所属的属性值子集作贡献**。而样本含缺失值时，它会**以不同的概率对所有属性值子集作贡献**。



## 6. 分类边界—多变量决策树

​	如果我们把每个属性当做坐标系的一个坐标轴，则d个属性描述的样本就对应了d维空间里的一个数据点。

对样本分类意味着寻找分类边界，决策树的边界就是都与坐标轴平行的边界

![](./images/决策树决策边界.png)

如果我们可以用斜线甚至非线性进行划分那么我们就可以更好的分类

- **多变量决策树**

试图寻找一个**最优的多属性的线性组合**作为节点，它的每个非叶节点都是一个形如 $\sum_{i=1}^d w_ia_i = t$ 的线性分类器。多变量决策树的决策边界能够**斜着走**，甚至**绕曲线走**，从而用更少的分支更好地逼近复杂的真实边界。

![](./images/多变量决策树.png)



# python代码

## sklearn

- **class sklearn.tree.DecisionTreeClassifier**  决策树分类
- **(criterion=’gini’,                          splitter=’best’,                  max_depth=None, min_samples_split=2,                     min_samples_leaf=1, min_weight_fraction_leaf=0.0,                                   max_features=None, random_state=None,                                               max_leaf_nodes=None, min_impurity_decrease=0.0,                                 min_impurity_split=None, class_weight=None, presort=False)**

- 参数
  - **Criterion**这个参数正是用来决定不纯度的计算方法的。sklearn提供了两种选择
    - 输入”entropy“，使用信息熵（Entropy）
    - 输入”gini“，使用基尼系数（Gini Impurity）
  - **splitter**特征划分点选择标准
    - **best**寻找最优划分点
    - **random**随机划分点 适合数据量大的时候
  - **max_features**划分时考虑的最大特征数
    - 默认是"None"，表示分时考虑所有的特征数
    - **"log2"**意味着划分时最多考虑log2N个特征
    - **sqrt"或者"auto"**意味着划分时最多考虑N−−√个特征。
    - 如果是整数，代表考虑的特征绝对数
    - 如果是浮点数，代表考虑特征百分比，即考虑（百分比xN）取整后的特征数。其中N为样本总特征数。
  - **max_depth**决策树最大深度，默认是无限，一般不会设置，如果设置则10-100之间
  - **min_samples_split**内部节点再划分所需最小样本数，这个值限制了子树继续划分的条件，如果某节点的样本数少于min_samples_split，则不会继续再尝试选择最优特征来进行划分。 默认是2.
  - **min_samples_leaf**叶子节点最少样本数，这个值限制了叶子节点最少的样本数，如果某叶子节点数目小于样本数，则会和兄弟节点一起被剪枝。 默认是1,
  - **min_weight_fraction_leaf**叶子节点最小的样本权重和，如果确实值很多要划分权重就要设置这个值
  - **max_leaf_nodes**最大叶子节点数
    - 通过限制最大叶子节点数，可以防止过拟合，默认是"None”，即不限制最大的叶子节点数。
  - **class_weight**类别权重
    - 如果使用“balanced”，则算法会自己计算权重，样本量少的类别所对应的样本权重会高。默认是None
  - **min_impurity_split**节点划分最小不纯度
    - 这个值限制了决策树的增长，如果某节点的不纯度(基尼系数，信息增益，均方差，绝对差)小于这个阈值，则该节点不再生成子节点
  - **presort**数据是否预排序
    - 这个值是布尔值，默认是False不排序

- 
  属性：
  - **feature_importances_** : array of shape = [n_features]
    -  特征重要性。该值越高，该特征越重要。
    -   特征的重要性为该特征导致的评价准则的（标准化的）总减少量。它也被称为基尼的重要性
  - **max_feature_**:int
    - max_features推断值。
  - **n_features_**：int
    - 执行fit的时候，特征的数量。
  - **n_outputs_** : int
    -  执行fit的时候，输出的数量。
  - **tree_** : 底层的Tree对象。
  - **__outputs_**_ : int
    -  执行fit后，输出的数量
- **方法：**
  - **fit(X,y)**:训练模型。
  - **predict(X**):预测
  - **predict_log_poba(X)**:预测X为各个类别的概率对数值。
  - **predict_proba(X)**:预测X为各个类别的概率值。





- **class sklearn.tree.DecisionTreeRegressor**回归决策树，参数和分类决策树相似，是用决策树的方法来做回归问题拟合曲线。

- **可视化树**

```python
# 可视化
clf = tree.DecisionTreeClassifier(criterion='entropy')

# 生成一个pdf
import graphviz
dot_data = tree.export_graphviz(clf, out_file=None)
graph = graphviz.Source(dot_data)
graph.render("iris")
```



```python
from sklearn.datasets import load_iris
from sklearn import tree
import pandas as pd

iris = load_iris()
table = pd.DataFrame(iris['data'])
table['target'] = iris['target']    # target => ['setosa', 'versicolor', 'virginica']
table.columns = ['sepal length (cm)','sepal width (cm)','petal length (cm)','petal width (cm)', 'target']


# 创建决策树对象，使用信息熵作为依据
clf = tree.DecisionTreeClassifier(criterion='entropy')
# fit方法分类。features为iris.data，labels为iris.target
clf = clf.fit(iris.data, iris.target)



dot_data = tree.export_graphviz(clf, out_file=None,
                         feature_names=iris.feature_names,
                         class_names=iris.target_names,
                         filled=True, rounded=True,
                         special_characters=True)
import  graphviz
graph = graphviz.Source(dot_data)
graph.render("iris")
```

