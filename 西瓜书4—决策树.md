# 决策树

[TOC]



## 1.基本流程

- **决策树**

**决策树（decision tree）**是一种模仿人类决策的学习方法。举个例子，比方说买电脑，我们首先看看外观帅不帅气，然后再看看性能怎么样，还得看看价格如何，最终**经过一系列的判断做出**是否购买电脑的**决策**。

一棵决策树可以分成三个部分：叶节点，非叶节点，分支。**叶节点**对应**决策结果**，也即分类任务中的类别标记；**非叶节点**（包括根节点）对应一个**判定问题**（某属性=？）；**分支**对应父节点**判定问题的不同答案**（可能的属性值），可能连向一个非叶节点的子节点，也可能连向叶节点。

决策就是从根节点开始走到叶节点的过程。每经过一个节点的判定，数据集就按照答案（属性值）划分为若干子集，**在子节点做判定时只需要考虑对应的数据子集就可以了**。

决策树学习的目的是为了**产生一棵泛化能力强，即处理未见示例能力强的决策树**。



- **决策树是一个递归过程**

生成算法：

1. 传入训练集和属性集
2. 生成一个新节点
3. 若此时数据集中所有样本都属于同一类，则把新节点设置为该类的叶节点，然后**返回**$^1$。
4. 若此时属性集为空，或者数据集中所有样本在属性集余下的所有属性上取值都相同，无法进一步划分，则把新节点设置为叶节点，类标记为数据集中样本数最多的类，然后**返回**$^2$
5. 从属性集中选择一个最优划分属性
   - 为该属性的每个属性值生成一个分支，并按属性值划分出子数据集
   - 若分支对应的子数据集为空，无法进一步划分，则直接把子节点设置为叶节点，类标记为父节点数据集中样本数最多的类，然后**返回**$^3$
   - 将子数据集和去掉了划分属性的子属性集作为算法的传入参数，继续生成该分支的子决策树。

稍微注意以下，3处返回中的第2处和第3处设置叶节点的类标记原理有所不同。第2处将类标记设置为当前节点对应为数据集中样本数最多的类，这是利用当前节点的**后验分布**；第3处将类标记设置为为父节点数据集中样本数最多的类，这是把父节点的样本分布作为当前节点的**先验分布**。

## 2.  划分选择—信息熵

决策树中，我们判定的初衷是希望划分后需要考虑的可能性更少，或者说希望子节点的纯度更高，混轮程度更低，（确定性越高）

- **信息熵**

是一种衡量样本集纯度的常用指标：熵就是混乱程度，不确定性

熵越大，混乱程度越高，不确定性越高

熵越小，混轮程度越低，确定性越高，更能确定是哪一类

$$Ent(D) = -\sum_{k=1}^{|\mathcal{Y}|}p_klog_2p_k$$

**一定要记得最前面的负号！！！**其中 $|\mathcal{Y}|$ 为类别集合，$p_k$ 为该类样本占样本总数的比例。



- **信息增益，ID3算法**

$$Gain(D,a) = Ent(D) - \sum_{v=1}^{V}\frac{|D^v|}{|D|}Ent(D^v)$$

前半项是信息熵

后一项表示用a这个属性划分能产生V个节点，那么这个划分Dv的信息熵的权重为多少

整个公式表示**信息增益越大，代表用属性 $a$ 进行划分所获得的纯度提升越大**。

其中 $V$ 表示属性 $a$ 的属性值集合，

$D^v$ 表示属性值为 $v$ 的数据子集。

求和项也称为**条件熵**，

我们可以理解为**它是先求出每个数据子集的信息熵，然后按每个数据子集占原数据集的比例来赋予权重，比例越大，对提升纯度的帮助就越大。**



- **互信息**信息增益又称为**互信息（Mutual information）**。
  - 一个连续变量X的不确定性，用方差Var(X)来度量
  - 一个离散变量X的不确定性，用熵H(X)来度量
  - 两个连续变量X和Y的相关度，用协方差或相关系数来度量
  - 两个离散变量X和Y的相关度，用互信息I(X;Y)来度量(直观地，X和Y的相关度越高，X对分类的作用就越大)



- **信息熵例子**

![](./images/信息熵1.png)

![](./images/信息熵2.png)

![](./images/信息熵3.png)



## 3. 其他划分—增益率，基尼系数

事实上在上面的例子中，西瓜的编号也是属性，也应该计算它的信息熵，结果为0.998，远大于其他属性，说明编号是个很确定的分类方法（因为每个编号只有一个样本，非常的确定不混乱）。

所以用信息增益不具有泛化能力，对那些能分出很多分支的属性有所偏好（因为他们信息增益大，熵降低的多）

- **增益率 C4,5算法**

其中，

$$IV(a) = -\sum_{v=1}^V\frac{|D^v|}{|D|}log_2\frac{|D^v|}{|D|}$$

**一定要记得最前面的负号！！！**IV称为属性的**固有值（intrinsic value）**

**属性a的取值树木越多，V就越大，衡量的是样本集在某个属性上的混乱程度**

增益率准则对可取值**数目较少的属性有所偏好**因此， C4.5
算法并不是直接选择增益率最大的候选划分属性，而是使用了一个启发式，先从候选划分属性中找出信息增益高于平均水平的属性，再从中选择增益率最高的.



- **基尼系数，CART算法**

基尼值：

$$Gini(D) = \sum_{k=1}^{|\mathcal{Y}|}\sum_{k' \neq k}p_kp_{k'}\\
=1-\sum_{k=1}^{|\mathcal{Y}|}p_k^2$$

基尼指数：

$$Gini\_index(D,a) = \sum_{v=1}^{V}\frac{|D^v|}{|D|}Gini(D^v)$$

反映的是**从一个数据集中随机抽取两个样本，其类别标志不同的概率**。

**基尼值越小，样本集的纯度越高**。

于是，我们在候选属性集合A 中，选择那个使得划分后基尼指数最小的属性作为最优划分属性



## 4. 剪枝处理

为了防止过拟合，分支太多的话不利于模型泛化，需要去掉一些分支

- **预剪枝**

往下分支之前，先把这个节点当做叶节点不划分，用验证集来计算验证精度， 划分之后再计算精度，如果划分后的精度高于不划分，则可以划分，否则就不划分

![](./images/预剪枝.png)

- **后剪枝**

先生成完整的决策树，然后从底部开始逐个考察节点，如果不展开节点的精度比展开后的精度高，则删去这个节点下面的分支。

若精度相同，也会剪枝保证精简模型

- **优缺点**

预剪枝是一种贪心策略，降低了过拟合风险，同时也**显著减少了模型的训练时间开销和测试时间开销**。但是！这种贪心策略有可能导致**欠拟合**

后剪枝是种比较保守的策略，**欠拟合的风险很小，泛化性能往往优于预剪枝的决策树**。但是由于后剪枝是在生成了完整决策树后，自底向上对所有非叶节点进行考察，所以**训练时间开销要比未剪枝决策树和预剪枝决策树都大得多**。



## 5连续值和缺失值

- **连续值属性**

有时候会碰到属性是连续而非离散的情况。我们用**二分法**来离散化这个连续属性

给定一个包含连续属性 $a$ 的数据集，并且 $a$ 在数据集中有 $n$ 个不同取值，我们先把属性 $a$ 的 $n$ 个属性值**从小到大进行排序**。**所谓“二分”是指将这些属性值分为两个类别**（比方说把身高这一属性分为高于170和低于170两个类别）。

在对连续属性值排序完之后，由于有 $n$ 个不同取值，取每**两个取值的平均值作为划分点**的话，就有 $n-1$ 个候选划分点。我们需要做得就是按照准则（比方说用ID3算法的话就是信息增益）进行 $n-1$ 次判断。每次拿出一个候选划分点，把连续属性分为两类，转换为离散属性。然后基于这个基础计算准则，**最终选出一个最优的属性值划分点。**

注意！和离散属性不同，连续属性用于当前节点的划分后，其**后代节点依然可以使用该连续属性进一步划分**。比方说当前节点用身高低于170划分了，那么它的后代节点还可以用身高低于160来进一步划分。



- **缺失值**

假设数据集为 $D$，有缺失值的属性为 $a$，令 $\tilde{D}$ 表示 $D$ 中没有缺失属性 $a$ 的样本子集。我们只能基于 $\tilde{D}$ 来判断属性 $a$ 的优劣。但是我们又希望包含缺失值的样本也能在建模过程体现出一定的影响了，因此要**重新定义准则**。在那之前，先定义几个新定义用到的变量：

$$\rho = \frac{\sum_{\mathbf{x} \in \tilde{D}}w_\mathbf{x}}{\sum_{\mathbf{x} \in D}w_\mathbf{x}}$$

$$\tilde{p_k} = \frac{\sum_{\mathbf{x} \in \tilde{D_k}}w_\mathbf{x}}{\sum_{\mathbf{x} \in \tilde{D}}w_\mathbf{x}},\quad (1 \leq k \leq |\mathcal{Y}|)$$

$$\tilde{r_v} = \frac{\sum_{\mathbf{x} \in \tilde{D^v}}w_\mathbf{x}}{\sum_{\mathbf{x} \in \tilde{D}}w_\mathbf{x}},\quad (1 \leq v \leq V)$$

$\rho$ 表示**无缺失值样本所占的比例**;

$\tilde{p_k}$ 表示**无缺失值样本中第 $k$ 类所占的比例**;

$\tilde{r_v}$ 表示**无缺失值样本中在属性 $a$ 上取值 $a^v$ 的样本所占的比例** ;

注意，这里的 $w_\mathbf{x}$ 表示样本的权值，它是**含缺失值样本参与建模**的一种方式。在根节点处初始时，所有样本 $\mathbf{x}$ 的权重都为1。

接下来重新定义信息熵和信息增益，推广到样本含缺失值的情况：

$$Ent(\tilde{D}) = -\sum_{k=1}^{|\mathcal{Y|}}\tilde{p_k}log_2\tilde{p_k}$$

$$Gain(D,a) = \rho \times Gain(\tilde{D},a)\\
= \rho \times (Ent(\tilde{D}) - \sum_{v=1}^V\tilde{r_v}Ent(\tilde{D^v}))$$

按照新的定义来计算包含缺失值的属性的信息增益，然后和其他属性的信息增益相比，选出最优的。

- **给定划分属性，如何划分缺失该属性值的样本？**

假设有一个包含缺失值的属性被计算出是最优划分属性，那么我们就要按该属性的不同取值划分数据集了。缺失该属性值的样本怎么划分呢？答案是**按概率划分**，这样的样本会被**同时划入所有子节点**，并且其**权重更新**为对应的 $\tilde{r_v} \dot w_\mathbf{x}$。

可以把无缺失值的决策树建模想象为各样本权值恒为1的情形，它们**只对自己所属的属性值子集作贡献**。而样本含缺失值时，它会**以不同的概率对所有属性值子集作贡献**。



## 6.回归决策树

回归树是可以用于回归的决策树模型，**一个回归树对应着输入空间（即特征空间）的一个划分以及在划分单元上的输出值**.与分类树不同的是，回归树对输入空间的划分采用一种启发式的方法，会遍历所有输入变量，找到最优的切分变量jj和最优的切分点ss，即选择第jj个特征xjxj和它的取值ss将输入空间划分为两部分，然后重复这个操作。
而如何找到最优的jj和ss是通过比较不同的划分的误差来得到的。一个输入空间的划分的误差是用真实值和划分区域的预测值的最小二乘来衡量的，即

![](./images/回归决策树.png)

举个例子，我们要对南京市各地区的房价进行回归预测，我们将输入空间不断的按最小误差进行划分，得到类似下图的结果，将空间划分后，我们会用该单元内的均值作为该单元的预测值，如图中一片区域的平均房价作为该划分单元内房价的预测值(唔，怎么感觉这个例子还是不太准确…）



## 7. 分类边界—多变量决策树

​	如果我们把每个属性当做坐标系的一个坐标轴，则d个属性描述的样本就对应了d维空间里的一个数据点。

对样本分类意味着寻找分类边界，决策树的边界就是都与坐标轴平行的边界

![](./images/决策树决策边界.png)

如果我们可以用斜线甚至非线性进行划分那么我们就可以更好的分类

- **多变量决策树**

试图寻找一个**最优的多属性的线性组合**作为节点，它的每个非叶节点都是一个形如 $\sum_{i=1}^d w_ia_i = t$ 的线性分类器。多变量决策树的决策边界能够**斜着走**，甚至**绕曲线走**，从而用更少的分支更好地逼近复杂的真实边界。

![](./images/多变量决策树.png)



# python代码

## sklearn

- ## 7.决策树

  ```python
  sklearn.tree.DecisionTreeClassifier
  (*, criterion='gini', splitter='best', max_depth=None, min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features=None, random_state=None, max_leaf_nodes=None, min_impurity_decrease=0.0, min_impurity_split=None, class_weight=None, ccp_alpha=0.0
  ```

  - **参数**
    - **criterion: {“gini”, “entropy”}, default=”gini”**
      - 算法，用来分支的计算方法
      - ”gini” 默认是基尼系数，表示在这个属性下能选出不同类的概率
      - “entropy” 信息熵，混乱程度
    - **splitter ; {“best”, “random”}, default=”best”**
      - 特征划分点选择不熬转
      - “best” 寻找最优划分
      - “random” 随机划分
    - **max_depth ：int, default=None**
      - 决策树的最大深度
    - **min_samples_split ; int or float, default=2**
      - 节点再划分时候所需的最小样本数
      - 默认是2，如果传入int则整数作为最小样本数
      - 如果为float，`min_samples_split`则为分数意思为百分比，用该份数乘以样本数的结果作为最小样本数
    - **min_samples_leaf ： int or float, default=1**
      - 在叶节点处需要的最小样本数
      - 如果为int，则认为`min_samples_leaf`是最小值。
      - 如果为float，`min_samples_leaf`则为分数， 是每个节点的最小样本数。`ceil(min_samples_leaf * n_samples)`
    - **min_weight_fraction_leaf：：float, default=0.0**
      - 在所有叶节点处（所有输入样本）的权重总和中的最小加权分数。如果未提供sample_weight，则样本的权重相等。
    - **max_features： int, float or {“auto”, “sqrt”, “log2”}, default=None**
      - 划分时考虑的最大特征数
      - 默认是"None"，表示分时考虑所有的特征数
      - 如果是整数，代表考虑的特征绝对数
      - 如果是浮点数，代表考虑特征百分比，即考虑（百分比xN）取整后的特征数。其中N为样本总特征数。
      - "log2"意味着划分时最多考虑log2N个特征
      - sqrt"或者"auto"意味着划分时最多考虑根号下N个特征。
    - **random_state ： int, RandomState instance or None, default=None**
      - 算法将在每个拆分中随机选择，然后找到它们之间的最佳拆分。但是最佳发现的拆分可能会因不同的运行而有所不同
      - 如果是int，则random_state是随机数生成器使用的种子; 
      - 如果是RandomState实例，则random_state是随机数生成器; 
      - 如果为None，则随机数生成器是np.random使用的RandomState实例。
    - **max_leaf_nodes： int, default=None**
      - 通过限制最大叶子节点数，可以防止过拟合，默认是"None”，即不限制最大的叶子节点数。
    - **min_impurity_decrease ：float, default=0.0**
      - 如果节点分裂会导致混乱的减少大于或等于该值，则该节点将被分裂
    - **min_impurity_split ：float, default=0**
      - 节点划分最小不纯度，这个值限制了决策树的增长，如果某节点的不纯度(基尼系数，信息增益，均方差，绝对差)小于这个阈值，则该节点不再生成子节点
    - **class_weight*****dict, list of dict or “balanced”, default=None**
      - 与形式的类有关的权重。如果为None，则所有类的权重都应为1。对于多输出问题，可以按与y列相同的顺序提供字典列表
        - 注意，应当为onehot编码的编号1处都定义权重，比如四分类 [{0: 1, 1: 1}, {0: 1, 1: 5}, {0: 1, 1: 1}, {0: 1, 1: 1}] 而不是 [{1:1}, {2:5}, {3:1}, {4:1}].
      - 如果使用“balanced”，则算法会自己计算权重，样本量少的类别所对应的样本权重会高。默认是None
      - 如果指定了sample_weight，则这些权重将与sample_weight（通过fit方法传递）相乘
    - **ccp_alpha ; non-negative float, default=0.0**
      - 最小剪枝系数，默认为0
      - 该参数用来限制树过拟合的剪枝参数,，模型将会选择小于输入值最大α ,ccp_alpha=0时，决策树不剪枝；ccp_alpha越大，越多的节点被剪枝。Scikit learn 0.22版本新增参数。

  - **属性**
    - **classes_** ： 形状（n_classes）的ndarray或ndarray的列表**
      - 类标签
    - **feature_importances_ ： ndarray of shape (n_features,)**
      - 特征重要性。该值越高，该特征越重要。
      - 特征的重要性为该特征导致的评价准则的（标准化的）总减少量。它也被称为基尼的重要性
    - **max_features_： int**
      - 最大迭代次数
    - **n_classes_ ： int or list of int**
      - 类的数量
    - **n_features_ ：int**
      - 执行fit的时候，特征的数量。
    - **tree_： Tree instance**
      - 底层的Tree对象。
  - **方法**
    - **apply（X）**
      - 返回每个样本被预测的叶子的索引
    - **cost_complexity_pruning_path（X，y，sample_weight=None)**
      - 计算修剪路径
    - **decision_path（X，check_input=True)**
      - 返回决策路径，是个稀疏矩阵
    - **fit(*X*, *y*, *sample_weight=None*)**
      - 训练分类器模型
    - **fit_predict(X,y)**
      - 训练模拟器并预测每个样本的聚类
    - **get_depth()**
      - 返回决策树的深度
    - **get_n_leaves()**
      - 返回决策树的叶子数
    - **get_params(deep=True)**
      - deep ： bool 默认为True
      - 返回字典，估计器的各个设置参数
    - **predict（X）**
      - 用估计其预测X，得到预测值
    - **predict_log_proba（X）**
      - 预测X的类对数概率
    - **predict_proba（X）**
      - 预测X的概率
    - **score(X,y,sample_weight)：**
      - 返回（X，y）上的的平均准确度
    - **set_params()**
      - 该估计其的设置

  - **备注**

  控制树（例如`max_depth`，`min_samples_leaf`等）大小的参数的默认值会导致树完全生长和未修剪，这在某些数据集上可能非常大。为了减少内存消耗，应通过设置这些参数值来控制树的复杂性和大小。



- **class sklearn.tree.DecisionTreeRegressor**回归决策树，参数和分类决策树相似，是用决策树的方法来做回归问题拟合曲线。

- **可视化树**

```python
# 可视化
clf = tree.DecisionTreeClassifier(criterion='entropy')

# 生成一个pdf
import graphviz
dot_data = tree.export_graphviz(clf, out_file=None)
graph = graphviz.Source(dot_data)
graph.render("iris")
```



```python
from sklearn.datasets import load_iris
from sklearn import tree
import pandas as pd

iris = load_iris()
table = pd.DataFrame(iris['data'])
table['target'] = iris['target']    # target => ['setosa', 'versicolor', 'virginica']
table.columns = ['sepal length (cm)','sepal width (cm)','petal length (cm)','petal width (cm)', 'target']


# 创建决策树对象，使用信息熵作为依据
clf = tree.DecisionTreeClassifier(criterion='entropy')
# fit方法分类。features为iris.data，labels为iris.target
clf = clf.fit(iris.data, iris.target)



dot_data = tree.export_graphviz(clf, out_file=None,
                         feature_names=iris.feature_names,
                         class_names=iris.target_names,
                         filled=True, rounded=True,
                         special_characters=True)
import  graphviz
graph = graphviz.Source(dot_data)
graph.render("iris")
```

