# 贝叶斯分类器

## 1.贝叶斯决策论

以多分类任务为例，假设有 $N$ 种标记，即 $\mathcal{Y} = {c_1, c_2,..., c_N}$，用 $\lambda_{ij}$ 表示把一个真实标记为 $c_i$ 的样本误分类为 $c_j$ 所产生的损失。那么将样本 $\mathbf{x}$ 分类为 $c_i$ 的**期望损失（expected loss）**或者说，在样本 $\mathbf{x}$ s上的**条件风险（conditional risk）**：

$$ R(c_i | \mathbf{x}) = \sum_{j=1}^N \lambda_{ij} P(c_j | \mathbf{x})$$

这个公式是个加和公式，每个损失乘以它出现的概率然后求和就是总损失

它描述的是，给定一个样本 $\mathbf{x}$，把它分类为 $c_i$ 需要冒多大的风险，或者说，当样本真实标记不是 $c_i$ 时，会有多大的损失。

在单个样本条件风险的基础上，可以定义**总体风险**：

$$ R(h) = \mathbb{E}_{\mathbf{x}}[R(h(\mathbf{x})\ |\ \mathbf{x})]$$

它描述的是，**所有样本的条件风险的数学期望**，h就是假设函数

我们的目标就是找出能**最小化总体风险 $R(h)$ 的判断准则**



- **贝叶斯判定准则（Bayes decision rule）**

要最小化总体风险，只需**在每个样本上选择能使对应的条件风险 $R(c\ |\ \mathbf{x})$ 最小的标记**。即：

$$h^*(\mathbf{x}) = \arg \min_{c \in \mathcal{Y}} R(c\ |\ \mathbf{x})$$

这个判断准则 $h^*$ 称为**贝叶斯最优分类器（Bayes optimal classifier）**，对应的总体风险 $R(h^*)$ 称为**贝叶斯风险（Bayes risk）**，而  $1-R(h^*)$ 则反映了分类器所能达到的最好性能，也即**模型精度的理论上限**（风险最小的情况下的概率）。



- **模型搭建**

![](./images/贝叶斯1.png)

学习模型的目标是**令分类错误率最小**，那么分类正确时误分类损失 $\lambda_{ij}$ 为0，反之为1

这时条件风险就是（根据上面的求和公式）：

[推导]：由公式(7.1)和公式(7.4)可得：
$$R(c_i|\boldsymbol x)=1*P(c_1|\boldsymbol x)+...+1*P(c_{i-1}|\boldsymbol x)+0*P(c_i|\boldsymbol x)+1*P(c_{i+1}|\boldsymbol x)+...+1*P(c_N|\boldsymbol x)$$
又$\sum_{j=1}^{N}P(c_j|\boldsymbol x)=1$，则：
			$$	R(c_i|\boldsymbol x)=1-P(c_i|\boldsymbol x)$$

要令风险最小，我们只需要选择使样本 $\mathbf{x}$ 后验概率最大的一个类别标记就可以了。

问题在于，**怎样获取后验概率呢？**



- **机器学习的两种模式**

事实上，从概率的角度来理解，机器学习的目标就是**基于有限的训练样本集尽可能准确地估计出后验概率**（当然，大多数机器学习技术无需准确估计出后验概率）。要实现这个目标，主要有两种策略：

1. 构建**判别式模型（discriminative models）**：给定样本 $\mathbf{x}$，直接对后验概率 $P(\mathbf{x}\ |\ c)$ 建模来预测 $c$。这类模型包括决策树、BP神经网络、支持向量机等等。
2. 构建**生成式模型（generative models）** ：给定样本 $\mathbf{x}$，先对联合概率分布 $P(\mathbf{x},c)$ 建模，然后再利用联合概率计算出后验概率 $P(c\ |\ \mathbf{x})$，也即 $P(c\ |\ \mathbf{x}) = \frac{P(\mathbf{x},c)}{P(\mathbf{x})}$。



- **贝叶斯决策**

又因为联合概率 $P(\mathbf{x},c) = P(c\ |\ \mathbf{x}) \times P(\mathbf{x}) = P(\mathbf{x}\ |\ c) \times P(c)$，由此，能得到**贝叶斯定理**：

$$P(c\ |\ \mathbf{x}) = \frac{P(\mathbf{x}\ |\ c) \times P(c)}{P(\mathbf{x})}​$$

在贝叶斯定理中，每个概率都有约定俗成的名称：

- $P(c\ |\ \mathbf{x})$ 是类标记 $c$ 相对于样本 $\mathbf{x}$ 的条件概率，也由于得自 $\mathbf{x}$ 的取值而被称作 $c​$ 的后验概率。
- $P(\mathbf{x}\ |\ c)$ 是样本 $\mathbf{x}$ 相对于类标记 $c$ 的**类条件概率（class-conditional probability）**，或称为**似然（likelihood）**，也由于得自 $c$ 的取值而被称作 $\mathbf{x}$ 的后验概率。
- $P(c)$ 是 $c$ 的先验概率（也称为边缘概率），之所以称为"先验"是因为它不考虑任何 $\mathbf{x}$ 方面的因素。在这里又称为**类先验（prior）概率**。
- $P(\mathbf{x})$ 是 $\mathbf{x}$ 的先验概率。在这里是用作归一化的**证据（evidence）因子**，与类标记无关。

有了贝叶斯定理，如何估计后验概率 $P(c\ |\ \mathbf{x})$ 的问题就转化为**如何计算类先验概率 $P(c)$ 和类条件概率 $P(\mathbf{x}\ |\ c)$ **了。

类先验概率 $P(c)$ 表示的是**样本空间中各类样本的比例**，根据大数定律，**当训练集包含足够多的独立同分布样本**时，类先验概率可以直接通过**训练集中各类样本出现的频率**进行估计。

类条件概率 $P(\mathbf{x}\ |\ c)$ 的情况就复杂多了，它涉及到类 $c$ 中**样本 $\mathbf{x}$ 所有属性的联合概率**，假设每个样本有 $d$ 个二值属性，那么可能的取值组合就多达 $2^d$ 个，这个数目可能**远多于训练集的规模**，也就意味着很多样本的取值没有在训练集中出现，所以**直接用训练集出现的频率进行估计是不可行的**。必须注意**未被观测到**和**出现概率为0**的区别。



## 2.极大似然估计

- **极大似然估计**

先**假定该类样本服从某种确定的概率分布形式**，然后再**基于训练集中的该类样本对假定的概率分布的参数进行估计**。

比方说假定该类样本服从高斯分布，那么接下来就是利用训练集中该类样本来估计高斯分布的参数——均值和方差。

具体来说，如果类 $c$ 的样本服从参数为 $\theta_c$（可能不止一个参数）的分布，那么我们从样本空间抽取到该类的某一个样本 $\mathbf{x}$ 的概率就是 $P(\mathbf{x}\ |\ \theta_c)$。使用 $D_c$ 来表示训练集中类 $c$ 的子集，可以定义数据集 $D_c$ 的**似然（likelihood）**为：

$$P(D_c\ |\ \theta_c) = \prod_{\mathbf{x} \in D_c} P(\mathbf{x}\ |\ \theta_c)​$$

由于**连乘操作容易造成下溢**，实际任务中通常使用**对数似然（log-likelihood）**代替：

$$LL(\theta_c) = \log P(D_c\ |\ \theta_c) = \sum_{\mathbf{x} \in D_c} \log P(\mathbf{x}\ |\ \theta_c)$$

所谓**极大似然估计（Maximum Likelihood Estimation，简称MLE）**就是**找出令似然最大的参数 $\theta_c$**。也即从 $\theta_c$ 的所有可能取值中找到一个**令所抽取样本出现的可能性最大**的值。

求解的过程也很简单，就是求似然函数的导数，令导数为0，得到**似然方程**，解似然方程得到最优解，也即该类样本分布的参数。

尽管极大似然估计能使我们求取类条件概率的过程变得相对简单，但它有最大的一个缺点就是：估计结果的**准确性严重依赖于所假设的概率分布形式是否符合潜在的真实数据分布**



## 3.朴素贝叶斯分类器

$$P(c\ |\ \mathbf{x}) = \frac{P(\mathbf{x}\ |\ c) \times P(c)}{P(\mathbf{x})}$$

估计后验概率 $P(c\ |\ \mathbf{x})$ 最大的一个难处是：类条件概率 $P(\mathbf{x}\ |\ c)$ 是所有属性上的联合概率，而多个属性的不同属性值组合并不一定全部囊括在训练集内，所以很难通过训练集估计。

- **朴素贝叶斯**

**属性条件独立性假设（attribute conditional independence assumption）**。也就是说，假设**所有属性相互独立，单独地对分类结果产生影响**。

基于这个假设，可以把类条件概率写成连乘的形式，因此贝叶斯定理可重写为：

$$P(c\ |\ \mathbf{x}) = \frac{P(\mathbf{x}\ |\ c) \times P(c)}{P(\mathbf{x})} = \frac{P(c)}{P(\mathbf{x})} \prod_{i=1}^{d} P(x_i\ |\ c) \qquad (1)$$

其中 $d$ 为属性数目， $x_i$ 为样本 $\mathbf{x}$ 在第 $i$ 个属性上的取值。

又因为 $P(\mathbf{x})$ 与类别无关，所以**朴素贝叶斯分类器的表达式**可以写为：

$$h(\mathbf{x}) = \arg \max_{c \in \mathcal{Y}} P(c) \prod_{i=1}^{d} P(x_i\ |\ c)$$

前面已经提到过，当训练集包含足够多独立同分布样本时，类先验概率 $P(c)$ 可以直接算出，也即训练集该类样本的数目占训练集规模的比例：

$$P(c) = \frac{|D_c|}{|D|} \qquad (2)$$

而条件概率 $P(x_i\ |\ c)$，根据属性类型分离散和连续两种情况：

- 离散型属性：条件概率 $P(x_i\ |\ c)$ 可以估计为，在类别 $c$ 的样本子集中，第 $i$ 个属性取值为 $x_i$ 的样本所占的比例：

$$P(x_i\ |\ c) = \frac{|D_{c,x_i}|}{|D_c|} \qquad (3)​$$

- 连续性属性：替换为概率密度函数，假设第 $i$ 个属性服从高斯分布，那么条件概率就写成 $p(x_i\ |\ c) \sim \mathcal{N}(\mu_{c,i},\sigma_{c,i}^2)$。我们利用类别 $c$ 的样本子集在该属性上的取值算出分布的均值和方差，然后把属性取值 $x_i$ 代入概率密度函数就可算出这个条件概率。



**例子**

![](./images/朴素贝叶斯.png)

## 4.平滑

注意了，若**某个属性值在训练集中没有与某个类同时出现过**，那么它对应的条件概率 $P(x_i\ |\ c)$ 就为0。在连乘中，这就意味着整个式子值为0了，**其他属性携带的信息都被抹去了**

例如，在使用西
瓜数据集3.0 训练朴素贝叶斯分类器时，对一个"敲声=情脆"的测试例，有

![](./images/平滑.png)因此，无论该样本的其他属性是什么，哪怕在其他属性上明显像好瓜，分类的结果都将是"好瓜：否“，这显然不太合理

- **平滑smoothing处理：拉普拉斯修正Laplacian correction**

假设**训练集中**包含 $N$ 个类别，第 $i$ 个属性包含 $N_i$ 种取值，则拉普拉斯修正把式（2）和式（3）修改为：

$$P(c) = \frac{|D_c| + 1}{|D| + N} \qquad (4)$$

$$P(x_i\ |\ c) = \frac{|D_{c,x_i}| + 1}{|D_c| + N_i} \qquad (5)​$$

![](./images/平滑2.png)

这样就不会出现某个属性没有出现过从而概率为0的情况了。



- **实际使用**
- 在实际任务中，有两种使用方式：
  - **查表**：若对预测速度要求较高，可以先根据训练集把所有涉及到的概率计算出来，然后存储好，在预测新样本时只需要查表然后计算就可以了。
  - **懒惰学习**：若数据更替比较频繁，也可以理解为用训练集算出的概率可能很快就失效了，更新换代的速度很快，那就采取**懒惰学习（lazy learning）**的方式，仅当需要预测时才计算涉及到的概率。

特别地，当我们采取了预先计算所有概率的方式时，如果有新数据加入到训练集，我们只需要更新新样本涉及到的概率（或者说计数）就可以了，可以很方便地实现**增量学习**。

## 3.半朴素贝叶斯分类器

朴素贝叶斯分类器基于属性条件独立性假设，但这个假设往往很难成立的，有时候**属性之间会存在依赖关系**，这时我们就需要对属性条件独立性假设适度地进行放松，**适当考虑一部分属性间的相互依赖信息**，这就是**半朴素贝叶斯分类器（semi-naive Bayes classifier）**的基本思想。

- **独依赖估计（One-Dependent Estimator，简称ODE）**

假设的是**每个属性在类别之外最多仅依赖于一个其他属性**。也即：

$$P(c\ |\ \mathbf{x}) \propto P(c) \prod_{i=1}^{d} P(x_i\ |\ c,{pa}_i)$$

现在问题转化为**如何确定每个属性的父属性**？



- **SPODE（Super-Parent ODE）**

所有属性都依赖于一个共同的属性，称为**超父（super-parent）**，比方说上图中的 $x_1$。可以通过交叉验证等模型选择方法来确定最合适的超父属性。

![](./images/SPODE.png)

- **TAN（Tree augmented naive Bayes）**则是一种基于**最大带权生成树（maximum weighted spanning tree）**的方法，包含以下四个步骤：
  1. 计算任意两个属性间的**条件互信息（conditional mutual information）**：<br>
     $$I(x_i,x_j\ |\ y) = \sum_{x_i,x_j; c\in \mathcal{Y}}  P(x_i,x_j\ |\ c) \log \frac{ P(x_i,x_j\ |\ c)}{ P(x_i\ |\ c) P(x_j\ |\ c)}$$
  2. 以属性为节点构建完全图，每条边的权重为对应的条件户信息。
  3. 构建此完全图的最大带权生成树。选一个节点作为根节点，把边转换为有向边。
  4. 加入类别节点 $y$，并增加从 $y$ 到每个属性的有向边。



- **AODE(Average One-Dependent Estimator)**

它基于集成学习机制。无须通过模型选择来确定超父属性，而是尝试将每个属性都作为超父属性来构建模型，然后把有足够训练数据支撑的SPODE模型集成起来导出最终结果。

![](./images/AODE.png)

不难看出，与朴素贝叶斯分类器类似， AODE 的训练过程也是"计数"，即
在训练数据集上对符合条件的样本进行计数的过程.



- **高阶依赖**

ODE假设每个属性最多依赖类别以外的另一属性，但如果我们继续放宽条件，**允许属性最多依赖类别以外的 k 个其他属性**，也即考虑属性间的**高阶依赖**，那就得到了 kDE。

是不是考虑了高阶依赖就一定能带来性能提升呢？并不是这样的。随着 k 的增加，要准确估计条件概率 $P(x_i\ |\ c,\mathbf{pa}_i)$ **所需的训练样本会呈指数上升**。如果训练样本不够，很容易陷入高阶联合概率的泥沼；但如果训练样本非常充足，那就有可能带来泛化性能的提升。



## 5.贝叶斯网


![](./images/贝叶斯网.png)

## 6.EM算法

前面讨论的极大似然估计方法是一种常用的参数估计方法，它是假设分布的形式，然后用训练样本来估计分布的参数。但实际任务中，我们遇到一个很大的问题就是**训练样本不完整**。这时就需要用到**EM（Expectation-Maximization）算法**了。

所谓不完整的样本，说的是这个样本某些属性的值缺失了。将每个属性的取值看为一个变量，那么缺失的就可以看作“未观测”变量，比方说有的西瓜根蒂脱落了，没办法看出根蒂是“蜷缩”还是“硬挺”，那么这个西瓜样本的根蒂属性取值就是一个“未观测”变量，更准确地说，称作**隐变量（latent variable）**。

整个训练集可以划分为已观测变量集 $X$ 和隐变量集 $Z$ 两部分。按照极大似然的思路，我们依然是想找出令训练集被观测到的概率最大的参数 $\Theta$。也即最大化对数似然：

$$LL(\Theta\ |\ X,Z) = \ln P(X,Z\ |\ \Theta)$$

但是，由于 $Z$ 是隐变量，我们没办法观测到，所以上面这个式子实际是没法求的。

怎么办呢？EM算法的思路很简单，步骤如下：

1. 设定一个初始的 $\Theta$
2. 按当前的 $\Theta$ 推断隐变量 $Z$ 的（期望）值
3. 基于已观测变量 $X$ 和 步骤2得到的 $Z$ 对 $\Theta$ 做最大似然估计得到新的 $\Theta$
4. 若未收敛（比方说新的 $\Theta$ 与旧的 $\Theta$ 相差仍大于阈值），就回到步骤2，否则停止迭代

EM算法可以看作是用**坐标下降（coordinate descent）**法来最大化对数似然下界的过程，每次固定 $Z$ 或者 $\Theta$ 中的一个去优化另一个，直到最后收敛到局部最优解。

理论上，用梯度下降也能求解带隐变量的参数估计问题，但按我的理解，由于隐变量的加入，使得求导这个步骤非常困难，计算也会随着隐变量数目上升而更加复杂，EM算法避免了这些麻烦。





# python

# sklearn 

在scikit-learn中，提供了3中朴素贝叶斯分类算法：**GaussianNB(高斯朴素贝叶斯)、MultinomialNB(多项式朴素贝叶斯)、BernoulliNB(伯努利朴素贝叶斯)**

简单介绍：

高斯朴素贝叶斯：适用于连续型数值，比如身高在160cm以下为一类，160-170cm为一个类，则划分不够细腻。

多项式朴素贝叶斯：常用于文本分类，特征是单词，值是单词出现的次数。

伯努利朴素贝叶斯：所用特征为全局特征，只是它计算的不是单词的数量，而是出现则为1，否则为0。也就是特征等权重。



- **sklearn.naive_bayes.GaussianNB(priors=None)** **高斯朴素贝叶斯**
- 属性
  - **priors属性**：获取各个类标记对应的先验概率
  - **class_prior_属性**：同priors一样，都是获取各个类标记对应的先验概率，区别在于priors属性返回列表，class_prior_返回的是数组
  - **theta_属性**：获取各个类标记在各个特征上的均值
  - **sigma_属性**：获取各个类标记在各个特征上的方差
- 方法
  - **get_params(deep=True)**：返回priors与其参数值组成字典
  - **set_params(**params)**：设置估计器priors参数
  - **fit(X, y, sample_weight=None)**：训练样本，X表示特征向量，y类标记，sample_weight表各样本权重数组
  - **partial_fit(X, y, classes=None, sample_weight=None)**：增量式训练，当训练数据集数据量非常大，不能一次性全部载入内存时，可以将数据集划分若干份，重复调用partial_fit在线学习模型参数，在第一次调用partial_fit函数时，必须制定classes参数，在随后的调用可以忽略
  - **predict(X)：**直接输出测试集预测的类标记
  - **predict_proba(X)**：输出测试样本在各个类标记预测概率值
  - **predict_log_proba(X)**：输出测试样本在各个类标记上预测概率值对应对数值
  - **score(X, y, sample_weight=None)**：返回测试样本映射到指定类标记上的得分(准确率)





- **sklearn.naive_bayes.MultinomialNB(alpha=1.0, fit_prior=True, class_prior=None)**  **多项式朴素贝叶斯：**
  - **alpha**：浮点型，可选项，默认1.0，添加拉普拉修/Lidstone平滑参数
  - **fit_prior**：布尔型，可选项，默认True，表示是否学习先验概率，参数为False表示所有类标记具有相同的先验概率
  - **class_prior**：类似数组，数组大小为(n_classes,)，默认None，类先验概率



- **属性**
  - **class_log_prior_**：各类标记的平滑先验概率对数值，其取值会受fit_prior和class_prior参数的影响
    - 若指定了class_prior参数，不管fit_prior为True或False，class_log_prior_取值是class_prior转换成log后的结果
    - 若fit_prior参数为False，class_prior=None，则各类标记的先验概率相同等于类标记总个数N分之一
    - 若fit_prior参数为True，class_prior=None，则各类标记的先验概率相同等于各类标记个数除以各类标记个数之和
  - **intercept**_：将多项式朴素贝叶斯解释的class_log_prior_映射为线性模型，其值和class_log_propr相同
  - **feature_log_prob_**：指定类的各特征概率(条件概率)对数值，返回形状为(n_classes, n_features)数组
  - **coef_：**将多项式朴素贝叶斯解释feature_log_prob_映射成线性模型，其值和feature_log_prob相同
  - **class_count_**：训练样本中各类别对应的样本数，按类的顺序排序输出
  - **feature_count_**：各类别各个特征出现的次数，返回形状为(n_classes, n_features)数组
- **方法**
  - **fit(X, y, sample_weight=None)**：根据X、y训练模型
  - **get_params(deep=True)**：获取分类器的参数，以各参数字典形式返回
  - **partial_fit(X, y, classes=None, sample_weight=None)**：对于数据量大时，提供增量式训练，在线学习模型参数，参数X可以是类似数组或稀疏矩阵，在第一次调用函数，必须制定classes参数，随后调用时可以忽略
  - **predict(X)：**在测试集X上预测，输出X对应目标值
  - **predict_log_proba(X)**：测试样本划分到各个类的概率对数值
  - **predict_proba(X)**：输出测试样本划分到各个类别的概率值
  - **score(X, y, sample_weight=None)**：输出对测试样本的预测准确率的平均值
  - **set_params(**params)**：设置估计器的参数





- **sklearn.naive_bayes.BernoulliNB(alpha=1.0, binarize=0.0, fit_prior=True,class_prior=None) 伯努利朴素贝叶斯，类似于多项式朴素贝叶斯，也主要用户离散特征分类，和MultinomialNB的区别是：MultinomialNB以出现的次数为特征值，BernoulliNB为二进制或布尔型特性**
  - **binarize：**将数据特征二值化的阈值
- 属性
  - **class_log_prior_：**类先验概率对数值，类先验概率等于各类的个数/类的总个数
  - **feature_log_prob_ :**指定类的各特征概率(条件概率)对数值，返回形状为(n_classes, n_features)数组
  - **class_count_**：按类别顺序输出其对应的个数
  - **feature_count_**：各类别各特征值之和，按类的顺序输出，返回形状为[n_classes, n_features] 的数组
- **方法**
  - 方法：同MultinomialNB的方法类似

