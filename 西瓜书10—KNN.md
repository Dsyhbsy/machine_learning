# KNN

[TOC]



## 1.k近邻算法

- **近邻** (k-Nearest Neighbor,  简称 kNN) 

是一种常用的**监督学习**方法.

工作机制:  给定测试样本,  基于**某种距离度量**找出训练集中与其**最靠近的 $k$ 个训练样本**,  然后基于这 **$k$ 个"邻居 "**的信息来进行**预测**.

**如何基于 $k$ 个"邻居"的信息进行预测:**   

- **在分类任务中:**  常用**"投票法"**,  即选择这 $k$ 个样本中出现**最多的类别标记**作为**预测结果**. 
- **在回归任务中:**  常用**"平均法"**,  即将这 $k$ 个样本的**实值输出标记的平均值**作为**预测结果**. 
- 还可以基于距离远近进行**加权平均**或**加权投票**,  **距离越近**的样本**权重越大**. 



k 近邻学习与前面的学习有一个很大的**不同之处**:  $k$ 近邻学习**没有显式的训练过程**. 

- **懒惰学习**(lazy learning):  此类学习技术在**训练阶段仅仅是把样本保存起来**,  训练时间开销为零,  **待收到测试样本后再进行处理**. 
- **急切学习**(eager learning):  在**训练阶段就对样本进行学习处理**的方法,  称为急切学习.





## 2.距离度量

- 在机器学习过程中，对于函数 dist(., .)*d**i**s**t*(.,.)，若它是一"距离度量" (distance measure)，则需满足一些基本性质:
  - 非负性：
  - 同一性：
  - 对称性： 
  - 直递性：

- **欧氏距离(Euclidean Distance)**

![](./images/欧氏距离.png)

- **曼哈顿距离(Manhattan Distance)**

![](./images/曼哈顿距离.png)

- **切比雪夫距离**

![](./images/切比雪夫距离.png)



- **闵可夫斯基距离(Minkowski Distance)**
- 其中p是一个变参数：
  - 当p=1时，就是曼哈顿距离；
  - 当p=2时，就是欧氏距离；
  - 当p→∞时，就是切比雪夫距离。

![](./images/闵氏距离.png)

- **闵可夫斯基距离(Minkowski Distance)**的几何图形

  - 其实和范数形式是一样的

  ![](./images/闵可夫斯基距离.png)





## 3.参数k的重要性

$k$ **是一个重要参数**,  

- 当 **$k$ 取不同值时**,  **分类结果会有显著不同**. 
- 当**采用不同的距离计算方式,**  则找出的"近邻"可能有显著差别,  从而**导致分类结果有显著不同**. 

图 10.1 给出了 $k$ 近邻分类器的一个示意图.

![](./images/knn参数k.png)

- **近似误差**
  - 对现有训练集的训练误差，**关注训练集**，
  - 如果**近似误差过小可能会出现过拟合的现象**，对现有的训练集能有很好的预测，但是对未知的测试样本将会出现较大偏差的预测。
  - 模型本身不是最接近最佳模型。
- **估计误差**
  - 可以理解为对测试集的测试误差，关注测试集，
  - 估计误差小说明对未知数据的预测能力好，
  - 模型本身最接近最佳模型。



**K值选择问题，李航博士的一书「统计学习方法」上所说：**

- 1) 选择较小的K值，就相当于用较小的领域中的训练实例进行预测，
  - “学习”近似误差会减小，只有与输入实例较近或相似的训练实例才会对预测结果起作用，与此同时带来的问题是“学习”的估计误差会增大，
  - 换句话说，**K值的减小就意味着整体模型变得复杂，容易发生过拟合；**
- 2) 选择较大的K值，就相当于用较大领域中的训练实例进行预测，
  - 其优点是可以减少学习的估计误差，但缺点是学习的近似误差会增大。这时候，**与输入实例较远（不相似的）训练实例也会对预测器作用，使预测发生错误。**
  - **且K值的增大就意味着整体的模型变得简单。**
- 3) K=N（N为训练样本个数），则完全不足取，
  - 因为此时无论输入实例是什么，都只是简单的预测它属于在训练实例中最多的类，模型过于简单，忽略了训练实例中大量有用信息。
- 在**实际应用中，K值一般取一个比较小的数值**，例如采用交叉验证法（简单来说，就是把训练数据在分成两组:训练集和验证集）来选择最优的K值。



## 4.K近邻的错误率

给定测试样本 $\boldsymbol x$ , 若其最近邻样本为 $\boldsymbol z$ ,  则最近邻分类器出错的概率就是 $\boldsymbol x$ 与 $\boldsymbol z$ 类别标记不同的概率,  即
$$
\color{red} P(e r r)=1-\sum_{c \in \mathcal{Y}} P(c | \boldsymbol{x}) P(c | \boldsymbol{z})\tag{10.1}
$$
假设样本独立同分布,  且对任意 $\boldsymbol x$ 和任意小正数 $\delta$ ,  在 $\boldsymbol x$ 附近 $\delta$ 距离范围内**总能找到一个训练样本**;  换言之,  对任意测试样本,  总能在任意近的范围内找到式 (10.1) 中的训练样本 $\delta$ . 令 $c^{*}=\arg \max _{c \in \mathcal{Y}} P(c | \boldsymbol{x})$ 表示贝叶斯最优分类器的结果,  有:  
$$
\color{red} \begin{eqnarray*} P(e r r) &=&1-\sum_{c \in \mathcal{Y}} P(c | \boldsymbol{x}) P(c | \boldsymbol{z}) \\ & \simeq& 1-\sum_{c \in \mathcal{Y}} P^{2}(c | \boldsymbol{x}) \\ & \leqslant& 1-P^{2}\left(c^{*} | \boldsymbol{x}\right) \\ &=&\left(1+P\left(c^{*} | \boldsymbol{x}\right)\right)\left(1-P\left(c^{*} | \boldsymbol{x}\right)\right) \\ & \leqslant& 2 \times\left(1-P\left(c^{*} | \boldsymbol{x}\right)\right)\tag{10.2} \end {eqnarray*}
$$

> ------
>
> 可以得到这样一个**结论**:
>
> 最近邻分类器虽简单,  但它的**泛化错误率不超过贝叶斯最优分类器**的**错误率**的**两倍**.  

## 5.kd树

实现k近邻算法时，**主要考虑的问题是如何对训练数据进行快速k近邻搜索。**

这在特征空间的维数大及训练数据容量大时尤其必要。

**k近邻法最简单的实现是线性扫描（穷举搜索），即要计算输入实例与每一个训练实例的距离。计算并存储好以后，再查找K近邻。**当训练集很大时，计算非常耗时。

为了提高kNN搜索的效率，可以考虑使用特殊的结构存储训练数据，以减小计算距离的次数。

- KD**树**

根据**KNN**每次需要预测一个点时，我们都需要计算训练数据集里每个点到这个点的距离，然后选出距离最近的k个点进行投票。**当数据集很大时，这个计算成本非常高，针对N个样本，D个特征的数据集，其算法复杂度为O（DN^2DN2）**。

**kd树**：为了避免每次都重新计算一遍距离，算法会把距离信息保存在一棵树里，这样在计算之前从树里查询距离信息，尽量避免重新计算。其基本原理是，**如果A和B距离很远，B和C距离很近，那么A和C的距离也很远**。有了这个信息，就可以在合适的时候跳过距离远的点。

这样优化后的算法复杂度可降低到O（DNlog（N））。感兴趣的读者可参阅论文：Bentley，J.L.，Communications of the ACM（1975）。

![](./images/KD树.png)





- **树的建立**

kd树(K-dimension tree)是**一种对k维空间中的实例点进行存储以便对其进行快速检索的树形数据结构。**kd树是一种二叉树，表示对k维空间的一个划分，**构造kd树相当于不断地用垂直于坐标轴的超平面将K维空间切分，构成一系列的K维超矩形区域**。kd树的每个结点对应于一个k维超矩形区域。**利用kd树可以省去对大部分数据点的搜索，从而减少搜索的计算量。**

（1）**构造根结点，使根结点对应于K维空间中包含所有实例点的超矩形区域；**

（2）**通过递归的方法，不断地对k维空间进行切分，生成子结点。**

（3）**上述过程直到子区域内没有实例时终止（终止时的结点为叶结点）**。

（4）通常，循环的选择坐标轴对空间切分，选择训练实例点在坐标轴上的中位数为切分点，这样得到的kd树是平衡的



- **最近邻域搜索（Nearest-Neighbor Lookup）**
  - .**顺着“搜索路径”找到最近邻的近似点**
  - .**回溯搜索路径**，并判断搜索路径上的结点的其他子结点空间中是否可能有距离查询点更近的数据点，如果有可能，则需要跳到其他子结点空间中去搜索
  - .**重复这个过程直到搜索路径为空**



## 6.代码接口

- **sklearn.neighbors.KNeighborsClassifier(n_neighbors=5, weights='uniform', algorithm='auto', leaf_size=30, p=2, metric='minkowski', metric_params=None, n_jobs=None, kwargs)**
  - **n_neighbors：**int,可选（默认= 5），k_neighbors查询默认使用的邻居数
  - **weights：**预测的权函数，概率值**。**

    -  ‘uniform’：同一的权重，即每个邻域中的所有点都是平均加权的。
    -  ‘distance’ ：这种情况下，距离越近权重越大，反之，距离越远其权重越小。

    -  [callable]（可调用）：用户定义的函数，它接受一个距离数组，并返回一个包含权重的相同形状的数组
  - **algorithm ：**用于计算最近邻居的算法,。有{‘auto’, ‘ball_tree’, ‘kd_tree’, ‘brute’}

    -  ‘auto’      ：根据样本数据自动刷选合适的算法。

    - ‘ball_tree’：构建“球树”算法模型。

    -  ‘kd_tree’ ：‘’kd树‘’算法。

    - ‘brute’     ：使用蛮力搜索，即或相当于Knn算法，需遍历所有样本数据与目标数据的距离，进而按升序排序从而选取最近的K个值，采用投票得出结果。
  - **leaf_size：**叶的大小，针对算法为球树或KD树而言。这个设置会影响构造和查询的速度，以及存储树所需的内存。最优值取决于问题的性质。
  - **metric：**用于树的距离度量。默认度量是Minkowski，p=2等价于标准的欧几里德度量。有关可用度量的列表，可以查阅距离度量类的文档。如果度量是“预先计算的”，则假定X是距离矩阵，在拟合期间必须是平方。
  - **p：**Minkowski度量参数的参数来自sklearn.emeics.pairwise.pairwise_距离。当p=1时，这等价于使用曼哈顿距离(L1)，欧几里得距离(L2)等价于p=2时，对于任意的p，则使用Minkowski_距离(L_P)。
  - **metric_params：**度量函数的附加关键字参数，设置应为dict（字典）形式。
  - **n_jobs：**要为邻居搜索的并行作业的数量。`None`指1，除非在 joblib.parallel_backend背景。`-1`意味着使用所有处理器，若要了解相关的知识应该具体查找一下。



- **属性**

  - **classes_***形状的数组（n_classes，）*分类器已知的类标签

  - **effective_metric_**使用的距离度量。与`metric`参数或其同义词相同，例如，如果`metric`参数设置为“ minkowski”且`p`参数设置为2，则为“ euclidean” 。

  - **effective_metric_params_***字典*度量功能的其他关键字参数。对于大多数指标，`metric_params`参数将与参数相同，但`p`如果`effective_metric_`属性设置为“ minkowski” ，则也可能包含 参数值。

  - **n_samples_fit_** *int*拟合数据中的样本数。

  - **outputs_2d**：`y`在拟合期间当形状为（n_samples，）或（n_samples，1）时为False，否则为True。

**方法：**

| fit(self, X[, y])                              | 以X为训练数据，y为目标值拟合模型   |
| ---------------------------------------------- | ---------------------------------- |
| get_params(self[, deep])                       | 获取此估计器的参数。               |
| kneighbors(self[, X, n_neighbors, …])          | 找到点的K邻域。                    |
| kneighbors_graph(self[, X, n_neighbors, mode]) | 计算X中点的k-邻域(加权)图          |
| predict(self, X)                               | 预测提供的数据的类标签             |
| predict_proba(self, X)                         | 返回测试数据X的概率估计。          |
| score(self, X, y[, sample_weight])             | 返回给定测试数据和标签的平均精度。 |
| set_params(self, \*\*params)                   | 设置此估计器的参数。               |







- **KNeighborsRegressor**   k近邻回归算法，同上