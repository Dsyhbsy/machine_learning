# 集成学习

[TOC]

## 1.个体与集成

- **个体学习器** (individual learner):  个体学习器通常由一个现有的学习算法从训练数据产生.
- **基学习器** (base learner):  若集成中只包含**同种类型**的**个体学习器**, 也即是**"同质"**的, 那么同质集成中的个体学习器称为**"基学习器".**  相应的学习算法称为"基学习算法".
- **组件学习器** (component learner):  若集成中包含**不同类型**的个体学习器,  也即是**"异质"**的,  则此时集成中的个体学习器称为**"组件学习器"**,  或者直接称为**个体学习器.**

集成学习通过将**多个学习器进行结合**,  常可获得**比单一学习器显著优越的泛化性能**.  特别是对"**弱学习器**" (弱学习器是指泛化性能略优于随机猜测的学习器) , 效果更加明显.  因此集成学习的很多理论研究都是针对弱学习器进行的,  而**基学习器有时也被直接称为弱学习器**.  但实际中, 考虑到其他因素, 往往会使用比较强的学习器. 

- **个体学习期的选择原则**
  - "**好而不同**", 
  - 个体学习器要有一定的<font color=red>"**准确性**"</font>,  **即学习器不能太坏**, 
  - 并且要有<font color=red>"**多样性**"</font>,  **即学习器见具有差异**.





- **集成学习的数学推导**

考虑二分类问题 $y \in\{-1,+1\}$ 和真实函数 $f$ ,  假定基分类器的错误率为 $\epsilon$ ,  即对每个基分类器 $h_{i}$ 有
$$
P\left(h_{i}(\boldsymbol{x}) \neq f(\boldsymbol{x})\right)=\epsilon\tag{8.1}
$$
假设集成通过**简单投票法**结合 $T$ 个基分类器,  若有**超过半数的基分类器正确**,  则**集成分类就正确**:
$$
H(\boldsymbol{x})=\operatorname{sign}\left(\sum_{i=1}^{T} h_{i}(\boldsymbol{x})\right)\tag{8.2}
$$

> ------
>
> **注2:**  对于二分类问题,   $y \in\{-1,+1\}$ ,  则预测标记 $h_{i}(\boldsymbol{x}) \in\{-1,+1\}$,  如果有一半分类正确, 那么 $\sum_{i=1}^{T} h_{i}(\boldsymbol{x})>0$ , 则 $\operatorname{sign}\left(\sum_{i=1}^{T} h_{i}(\boldsymbol{x})\right)=1$ , 即整体分类就正确, 
>
> 其中, $sign(x)$ 函数是符号函数, 当 $x>0$ 时, $sign(x)=1$ ; 当 $x=0$ 时, $sign(x)=0$ ;  当 $x<0$ 时,  $sign(x)=-1$ 
>
> $H(\boldsymbol x)$ **为整体分类函数**,  即**集成分类**

**假设基分类器的错误率相互独立**,  则由 Hoeffding 不等式可知,  集成错误率为
$$
\begin{aligned} P(H(\boldsymbol{x}) \neq f(\boldsymbol{x})) &=\sum_{k=0}^{\lfloor T / 2\rfloor}\left(\begin{array}{l}{T} \\ {k}\end{array}\right)(1-\epsilon)^{k} \epsilon^{T-k} \\ & \leqslant \exp \left(-\frac{1}{2} T(1-2 \epsilon)^{2}\right) \end{aligned}\tag{8.3}
$$
**结论:**

上式显示出,  <font color=red>**随着集成中个体分类器数目 $T$ 的增大,  集成的错误率将呈指数级下降,  最终趋向于零.** </font>

**注意:**

上面问题的分析是基于一个关键假设:  **基学习器的误差相互独立** 

现实很难满足, 实际上, 个体学习器的**"准确性"和"多样性"本身互为冲突**, 此消彼长.  一般情况下,  准确性很高之后,  要增加多样性就需牺牲准确性.



- **集成学习分类**

根据个体学习器的生成方式，目前的集成学习方法大致可分为两大类

1. 个体学习器问存在强依赖关系、必须串行生成的序列化方法**代表是Boosting**
2. 以及个体学习间不存在强依赖关系、可同时生成的并行化方法**。代表是Bagging 和"随机森林" (Random Forest).**
   

## 2 Boosting集成

- **Boosting集成**

Boosting 是一族可**将弱学习器提升为强学习器的算法** .  这族算法的**工作机制**类似于如下:

1. **先从初始训练集训练出一个基学习器**
2. **再根据基学习器的表现对训练样本分布进行调整,  使得先前基学习器做错的训练样本在后续受到更多关注**
3. **然后基于调整后的样本分布来训练下一个基学习器**
4. **如此重复进行,  直至基学习器数目达到事先指定的值 $T$ ,  最终将这 $T$ 个基学习器进行加权结合.**



- **AdaBoosting算法**
  - 总共有T轮
  - 每轮产生就学习器h之后都要验证是否比随机猜测的误差大
    - 误差大于随机猜测（二分类就是0.5）就停止
    - 误差小于就继续下一步
  - 更新权重
  - 更新数据集
  - 循环直到有T个基学习器

![](./images/boosting.png)

- **权重更新公式**

$$
\alpha_{t}=\frac{1}{2} \ln \left(\frac{1-\epsilon_{t}}{\epsilon_{t}}\right)\tag{8.11}
$$

- **数据集更新公式**

$$
\begin{eqnarray*} \mathcal{D}_{t+1}(\boldsymbol{x}) &=&\frac{\mathcal{D}(\boldsymbol{x}) e^{-f(\boldsymbol{x}) H_{t}(\boldsymbol{x})}}{\mathbb{E}_{\boldsymbol{x} \sim \mathcal{D}}\left[e^{-f(\boldsymbol{x}) H_{t}(\boldsymbol{x})}\right]} 
\\ &=&\frac{\mathcal{D}(\boldsymbol{x}) e^{-f(\boldsymbol{x}) H_{t-1}(\boldsymbol{x})} e^{-f(\boldsymbol{x}) \alpha_{t} h_{t}(\boldsymbol{x})}}{\mathbb{E}_{\boldsymbol{x} \sim \mathcal{D}}\left[e^{-f(\boldsymbol{x}) H_{t}(\boldsymbol{x})}\right]} 
\\ &=&\mathcal{D}_{t}(\boldsymbol{x}) \cdot e^{-f(\boldsymbol{x}) \alpha_{t} h_{t}(\boldsymbol{x})} \frac{\mathbb{E}_{\boldsymbol{x} \sim \mathcal{D}}\left[e^{-f(\boldsymbol{x}) H_{t-1}(\boldsymbol{x})}\right]}{\mathbb{E}_{\boldsymbol{x} \sim \mathcal{D}}\left[e^{-f(\boldsymbol{x}) H_{t}(\boldsymbol{x})}\right]}\tag{8.19} \end{eqnarray*}
$$



- Boosting 算法要求**基学习器**能**对特定的数据分布进行学习**

  - **重赋权法(re-weighting)**

    - 在训练过程的每一轮中,  根据样本分布为每个训练样本重新赋予一个权重.

    - Boosting 算法在训练的**每一轮都要检查**当前生成的**基学习器是否满足基本条件**,  一旦条件**不满足**,  则当前基学习器即被**抛弃**,  且学习过程**停止**.

      在此种情形下,  初始设置的学习轮数 $T$ 也许遥远未达到,  可能导致最终集成中只**包含很少的基学习器而性能不佳**.

  - **重采样法(re-sampling)** 

    - 对无法接受带权样本的基学习算法,  则可通过"重采样法" (re-sampling)来处理,  即在每一轮学习中,  根据样本分布对训练集重新进行采样,  再用重采样而得的样本集对基学习器进行训练
    - 可获得"重启动"机会以避免训练过程过早停止,  即在抛弃**不满足条件**的当前基学习器之后,  可根据当前分布**重新对训练样本进行采样**,  再基于新的采样结果**重新训练出基学习器**,  从而使得学习过程可以**持续到预设的 $T$ 轮**完成.  

​	

一般而言,  这两种做法没有显著的优劣差别.  

从偏差一方差分解的角度看， **Boosting 主要关住降低偏差**，因此Boosting
能基于泛化性能相当弱的学习器构建出很强的集成.



## 3. Bagging 与随机森林

由第一节得知，如果想得到泛化能力强的学习器，个体学习期应尽可能的**相互独立**

因此我们让基学习期有**尽可能大的差异**就能保证最大可能相互独立

一种可能的做法是对训练样本进行**采样**,  **产生出若干个不同的子集**,  再从每个数据子集中训练出**一个基学习器**.  

但是如果采样出的**每个子集完全不同**,  则每个基学习器只用到了一小部分训练数据,  甚至不足以进行有效学习,  这显然**无法确保产生出比较好的基学习器**. 

- **相互交叠采集**：

随机采样出m个样本之后再把这些样本放回去重新采集，，使得下次采样时该样本仍有可能被选中，这样，经过m次随机采样操作，我们得到含m 个样本的采样集，初始训练集中有的样本在采样集里多次出现，有的则从未出现.由式(2.1)可知，初始训练集中约有63.2%的样本出现在来样集中.

- **Bagging**

我们可以**采样出 $T$ 个含 $m$ 个训练样本的采样集**,  然后基于**每个采样训练集训练出一个基学习器**,  再将这些**基学习器进行结合**. 这就是 **Bagging 的基本流程**.

在对**预测输出**进行结合时,  Bagging 通常对**分类任务**使用**简单投票法**,  对**回归任务**使用**简单平均法**. 

![](./images/bagging.png)

**训练一个 Bagging 集成与直接使用基学习算法训练一个学习器的复杂度同阶.** 这说明 Bagging 是一个很高效的集成学习算法.

另外,  与标准 **AdaBoost** 只适用于**二分类任务**不同,  **Bagging** 能不经修改地用于**多分类**、**回归等任务**. 

从偏差方差分解的角度看， **Bagging 主要关注降低方差**，因此它在不剪枝
决策树、神经网络等易受样本扰动的学习器上效用更为明显

- **包外估计**

同时,  自助采样有 36.8% 的样本可用作验证集来对泛化性能进行**"包外估计".**

包外样本还有许多其他用途, 如:

1. 当基学习器是决策树时,  可使用包外样本来辅助剪枝,  或用于估计决策树中各结点的后验概率以辅助对零训练样本结点的处理.
2. 当基学习器是是神经网络时,  可使用包外样本来辅助早期停止以减小过拟合风险. 



## 4.随机森林(Random Forest）

- **随机森林**

随机森林 (Random Forest，简称 $RF$) 是 **Bagging 的一个扩展变体**.  $RF$ 在**以决策树为基学习器**构建 Bagging 集成的基础上,  进一步在决策树的训练过程中**引入了随机属性**选择. 



- **基学习期随机属性选择**

与决策树不同，决策树是在节点选择时候选择一个最优的属性能让混乱程度降低。

**随机森林**对基决策树的每个结点,  先从**该结点的属性集合**中**随机**选择一个**包含 $k$ 个属性**的**子集**,  然后再从这个**子集中**选择一个**最优属性**用于划分. 

这里 的参数 $k$ 控制了随机性的引入程度;

若令 $k = d$,   则基决策树的构建与传统决策树相同;

 若令 $k = 1$ ,  则是随机选择一个属性用 于划分;

  一般情况下,  推荐值 $k=\log _{2}d$



- **随机森林的优异性**

随机森林简单、容易 实现、计算开销小,  但它在很多现实任务中展现出**强大的性能**,  被誉为"**代表集成学习 技术水平的方法**" 

**样本扰动和属性扰动**。这就使得最终集成的**泛化性能**可通过个体学习器之间**差异度的增加**而进一步**提升**. 

![](./images/随机森林性能.png)

## 5.结合策略

- **学习期结合的好处**
  - 假设空间很大，用单一学习器可能会导致泛化能力不强
  - 会陷入局部最小，通过多次运行进行结合，可以降低陷入局部最小的风险
  - 从表示的方面来看，某些学习任务的真实假设可能不在当前学习算法所考虑的假设空间中，此时若使用单学习器则肯定无效，而通过结合多个学习器， 由于相应的假设空间有所扩大，有可能学得更好的近似

- **平均法**

对数值型输出 $h_{i}(\boldsymbol{x}) \in \mathbb{R}$ ,  最常见的结合策略是使用平均法 (averaging).

- 简单平均法(simple averaging)
  $$
  H(\boldsymbol{x})=\frac{1}{T} \sum_{i=1}^{T} h_{i}(\boldsymbol{x})\tag{8.22}
  $$

- 加权平均法(weighted averaging)
  $$
  H(\boldsymbol{x})=\sum_{i=1}^{T} w_{i} h_{i}(\boldsymbol{x})\tag{8.23}
  $$
  其中,  $w_{i}$ 是个体学习器 $h_{i}$ 的权重, 通常要求 $w_{i} \geqslant 0, \sum_{i=1}^{T} w_{i}=1$. 

加权平均法和简单平均法之间的关系:

简单平均法是加权平均法令 $w_{i}=1 / T$  的特例. 

事实上,  加权平均法可认为是集成学习研究的**基本出发点.**

加权平均法的**权重**一般是**从训练数据中学习**而得,  现实任务中的训练样本通常不充分或存在噪声,  这将使得学出的权重**不完全可靠**.  当集成规模较大时,  学习的权重比较多,  容易导致过拟合.

**加权平均法未必一起优于简单平均法**,  

**选择的原则:**

一般而言,  在个体**学习器性能相差较大**时宜使用**加权平均法**,  而在个体**学习器性能相近**时宜使用**简单平均法**.  



- **投票法**

对分类任务来说,  学习器 $h_{i}$ 将从类别标记集合 $\left\{c_{1}, c_{2}, \ldots, c_{N}\right\}$ 中预测出一个标记,  最常见的结合策略是使用**投票法** (voting) . 

将 $h_{i}$ 在样本 $\boldsymbol x$ 上的预测输出表示为一个 $N$ 维向量 $\left(h_{i}^{1}(\boldsymbol{x}) ; h_{i}^{2}(\boldsymbol{x}) ; \ldots ; h_{i}^{N}(\boldsymbol{x})\right)$ ,  其中 $h_{i}^{j}(\boldsymbol x)$ 是 $h_{i}$ 在类别标记 $c_{j}$ 上的输出.

- **绝对多数投票法**(majority voting)

$$
H(\boldsymbol{x})=\left\{\begin{array}{ll}{c_{j},} & {\text { if } \sum_{i=1}^{T} h_{i}^{j}(\boldsymbol{x})>0.5 \sum_{k=1}^{N} \sum_{i=1}^{T} h_{i}^{k}(\boldsymbol{x})} \\ {\text { reject, }} & {\text { otherwise. }}\end{array}\right.\tag{8.24}
$$

​        **即若某标记得票过半数,  则预测为该标记**;  否则拒绝预测.

- **相对多数投票法**(plurality voting)
  $$
  H(\boldsymbol{x})=c_{\underset{j}{\arg \max } \sum_{i=1}^{T} h_{i}^{j}(\boldsymbol{x})}\tag{8.25}
  $$

​       **即预测为的票数最多的标记,**  若同时有多个标记获得最高票,  则从中随机选取一个.

- **加权投票法**(weighted voting)
  $$
  H(\boldsymbol{x})=c_{\arg \max } \sum_{i=1}^{T} w_{i} h_{i}^{j}(\boldsymbol{x})\tag{8.26}
  $$
  与加权平均法类似,  $w_{i}$ 是 $h_{i}$ 的权重, 通常要求 $w_{i} \geqslant 0, \sum_{i=1}^{T} w_{i}=1$. 

需注意的是，若基学习器的类型不同?则
其类概率值不能直接进行比较;在此种情形下，通常可将类概率输出转化为类
标记输出(例如将类概率输出最大的时(x) 设为1 ，其他设为0) 然后再投票



- **学习法**

当训练数据很多时,  一种更为强大的结合策略是使用"学习法",  即通过另一个学习器来进行结合. Stacking 是学习法的典型代表.

在这个新数据集中,  **初级学习器的输出被当作样例输入特征**,  **而初始样本的标记仍被当作样例标记**. Stacking 的算法描述如图 8.9 所示. 同时,  初级集成是异质的.



- **多样性**

**适用回归学：个体学习器准确性越高、多样性越大，则集成越好**

- **多样性度量**

**适用分类：增加多样性：样本扰动，属性扰动，算法扰动，输出扰动**





# python代码

# sklearn

- **sklearn.ensemble . BaggingClassifier()** 
- **sklearn.ensemble.BaggingRegressor()**
  - **n_estimators：int, optional (default=10) 。 **  要集成的基估计器的个数。
  - **max_samples**： int or float, optional (default=1.0)。决定从x_train抽取去训练基估计器的样本数量。int 代表抽取数量，float代表抽取比例
  - **max_features** : int or float, optional (default=1.0)。决定从x_train抽取去训练基估计器的特征数量。int 代表抽取数量，float代表抽取比例
  - **base_estimator**：Object or None。None代表默认是DecisionTree，Object可以指定基估计器（base estimator）。
  - **bootstrap** : boolean, optional (default=True) 决定样本子集的抽样方式（有放回和不放回）
  - **bootstrap_features** : boolean, optional (default=False)决定特征子集的抽样方式（有放回和不放回）
  - **oob_score** : bool 决定是否使用包外估计（out of bag estimate）泛化误差
  - **warm_start** : bool, optional (default=False) true代表
  - 　**n_jobs** : int, optional (default=1) 
  - **random_state** : int, RandomState instance or None, optional (default=None)。如果int，random_state是随机数生成器使用的种子; 如果RandomState的实例，random_state是随机数生成器; 如果None，则随机数生成器是由np.random使用的RandomState实例。
  - **verbose** : int, optional (default=0) 

- **属性**
  - **estimators_** : list of estimators。The collection of fitted sub-estimators.
  - 　**estimators_samples_** : list of arrays
  - **estimators_features_** : list of arrays
  - **oob_score_** : float，使用包外估计这个训练数据集的得分。
  - **oob_prediction_** : array of shape = [n_samples]。在训练集上用out-of-bag估计计算的预测。 如果n_estimator很小，则可能在抽样过程中数据点不会被忽略。 在这种情况下，oob_prediction_可能包含NaN。







- **sklearn.ensemble.RandomForestClassifier**(n_estimators=10, criterion=’gini’, max_depth=None, bootstrap=True, random_state=None, min_samples_split=2)随机森林

- RandomForestRegressor

  - **n_estimators**：integer，optional(default = 10)森林里的树木数量120,200,300,500,800,1200

    - 在利用最大投票数或平均值来预测之前，你想要建立子树的数量。

  - **Criterion**：string，可选(default =“gini”)

    - 分割特征的测量方法

  - **max_depth**：integer或None，可选(默认=无)

    - 树的最大深度 5,8,15,25,30

  - **max_features="auto”**,每个决策树的最大特征数量

    - If "auto", then `max_features=sqrt(n_features)`.
    - If "sqrt", then `max_features=sqrt(n_features)`(same as "auto").
    - If "log2", then `max_features=log2(n_features)`.
    - If None, then `max_features=n_features`.

  - **bootstrap**：boolean，optional(default = True)

    - 是否在构建树时使用放回抽样

  - **min_samples_split** 内部节点再划分所需最小样本数

    - 这个值限制了子树继续划分的条件，如果某节点的样本数少于min_samples_split，则不会继续再尝试选择最优特征来进行划分，默认是2。
    - 如果样本量不大，不需要管这个值。如果样本量数量级非常大，则推荐增大这个值。

  - **min_samples_leaf** 叶子节点的最小样本数

    - 这个值限制了叶子节点最少的样本数，如果某叶子节点数目小于样本数，则会和兄弟节点一起被剪枝， 默认是1。

    - 叶是决策树的末端节点。 较小的叶子使模型更容易捕捉训练数据中的噪声。

    - > 一般来说，我更偏向于将最小叶子节点数目设置为大于50。

  - **min_impurity_split:** 节点划分最小不纯度

    - 这个值限制了决策树的增长，如果某节点的不纯度(基于基尼系数，均方差)小于这个阈值，则该节点不再生成子节点。即为叶子节点 。

    - > 一般不推荐改动默认值1e-7。





-  **sklearn.ensemble .AdaBoostClassifier**
- AdaBoostRegressor
  - **base_estimator：**AdaBoostClassifier和AdaBoostRegressor都有，即我们的弱分类学习器或者弱回归学习器。理论上可以选择任何一个分类或者回归学习器，不过需要支持样本权重。我们常用的一般是CART决策树或者神经网络MLP。默认是决策树
  - **algorithm**：这个参数只有AdaBoostClassifier有。主要原因是scikit-learn实现了两种Adaboost分类算法，SAMME和SAMME.R
    - SAMME使用了和我们的原理篇里二元分类Adaboost算法的扩展，即用对样本集分类效果作为弱学习器权重
    - SAMME.R使用了对样本集分类的预测概率大小来作为弱学习器权重。由于SAMME.R使用了概率度量的连续值，迭代一般比SAMME快，因此AdaBoostClassifier的默认算法algorithm的值也是SAMME.R
  - **loss**：这个参数只有AdaBoostRegressor有，
    - 有线性‘linear’, 平方‘square’和指数 ‘exponential’三种选择,
  - **n_estimators**： AdaBoostClassifier和AdaBoostRegressor都有，就是我们的弱学习器的最大迭代次数，
  - **learning_rate**:  AdaBoostClassifier和AdaBoostRegressor都有，即每个弱学习器的权重缩减系数νν
  - **max_features**: 划分时考虑的最大特征数
    - 可以使用很多种类型的值，默认是"None",意味着划分时考虑所有的特征数；如果是"log2"意味着划分时最多考虑log2Nlog2N个特征；如果是"sqrt"或者"auto"意味着划分时最多考虑N−−√N个特征。如果是整数，代表考虑的特征绝对数。如果是浮点数，代表考虑特征百分比，即考虑（百分比xN）取整后的特征数。
  - **max_depth**:决策树最大深
  - **min_samples_split**内部节点再划分所需最小样本数
  - **min_samples_leaf 叶子节点最少样本数**
  - **min_weight_fraction_leaf**：叶子节点最小的样本权重
  - **max_leaf_nodes**:  最大叶子节点数

