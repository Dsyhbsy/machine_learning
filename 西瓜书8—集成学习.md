# 集成学习

[TOC]

## 1.个体与集成

- **个体学习器** (individual learner):  个体学习器通常由一个现有的学习算法从训练数据产生.
- **基学习器** (base learner):  若集成中只包含**同种类型**的**个体学习器**, 也即是**"同质"**的, 那么同质集成中的个体学习器称为**"基学习器".**  相应的学习算法称为"基学习算法".
- **组件学习器** (component learner):  若集成中包含**不同类型**的个体学习器,  也即是**"异质"**的,  则此时集成中的个体学习器称为**"组件学习器"**,  或者直接称为**个体学习器.**

集成学习通过将**多个学习器进行结合**,  常可获得**比单一学习器显著优越的泛化性能**.  特别是对"**弱学习器**" (弱学习器是指泛化性能略优于随机猜测的学习器) , 效果更加明显.  因此集成学习的很多理论研究都是针对弱学习器进行的,  而**基学习器有时也被直接称为弱学习器**.  但实际中, 考虑到其他因素, 往往会使用比较强的学习器. 

- **个体学习期的选择原则**
  - "**好而不同**", 
  - 个体学习器要有一定的<font color=red>"**准确性**"</font>,  **即学习器不能太坏**, 
  - 并且要有<font color=red>"**多样性**"</font>,  **即学习器见具有差异**.





- **集成学习的数学推导**

考虑二分类问题 $y \in\{-1,+1\}$ 和真实函数 $f$ ,  假定基分类器的错误率为 $\epsilon$ ,  即对每个基分类器 $h_{i}$ 有
$$
P\left(h_{i}(\boldsymbol{x}) \neq f(\boldsymbol{x})\right)=\epsilon\tag{8.1}
$$
假设集成通过**简单投票法**结合 $T$ 个基分类器,  若有**超过半数的基分类器正确**,  则**集成分类就正确**:
$$
H(\boldsymbol{x})=\operatorname{sign}\left(\sum_{i=1}^{T} h_{i}(\boldsymbol{x})\right)\tag{8.2}
$$

> ------
>
> **注2:**  对于二分类问题,   $y \in\{-1,+1\}$ ,  则预测标记 $h_{i}(\boldsymbol{x}) \in\{-1,+1\}$,  如果有一半分类正确, 那么 $\sum_{i=1}^{T} h_{i}(\boldsymbol{x})>0$ , 则 $\operatorname{sign}\left(\sum_{i=1}^{T} h_{i}(\boldsymbol{x})\right)=1$ , 即整体分类就正确, 
>
> 其中, $sign(x)$ 函数是符号函数, 当 $x>0$ 时, $sign(x)=1$ ; 当 $x=0$ 时, $sign(x)=0$ ;  当 $x<0$ 时,  $sign(x)=-1$ 
>
> $H(\boldsymbol x)$ **为整体分类函数**,  即**集成分类**

**假设基分类器的错误率相互独立**,  则由 Hoeffding 不等式可知,  集成错误率为
$$
\begin{aligned} P(H(\boldsymbol{x}) \neq f(\boldsymbol{x})) &=\sum_{k=0}^{\lfloor T / 2\rfloor}\left(\begin{array}{l}{T} \\ {k}\end{array}\right)(1-\epsilon)^{k} \epsilon^{T-k} \\ & \leqslant \exp \left(-\frac{1}{2} T(1-2 \epsilon)^{2}\right) \end{aligned}\tag{8.3}
$$
**结论:**

上式显示出,  <font color=red>**随着集成中个体分类器数目 $T$ 的增大,  集成的错误率将呈指数级下降,  最终趋向于零.** </font>

**注意:**

上面问题的分析是基于一个关键假设:  **基学习器的误差相互独立** 

现实很难满足, 实际上, 个体学习器的**"准确性"和"多样性"本身互为冲突**, 此消彼长.  一般情况下,  准确性很高之后,  要增加多样性就需牺牲准确性.



- **集成学习分类**

根据个体学习器的生成方式，目前的集成学习方法大致可分为两大类

1. 个体学习器问存在强依赖关系、必须串行生成的序列化方法**代表是Boosting**
2. 以及个体学习间不存在强依赖关系、可同时生成的并行化方法**。代表是Bagging 和"随机森林" (Random Forest).**
   

## 2 Boosting集成

- **Boosting集成**

Boosting 是一族可**将弱学习器提升为强学习器的算法** .  这族算法的**工作机制**类似于如下:

1. **先从初始训练集训练出一个基学习器**
2. **再根据基学习器的表现对训练样本分布进行调整,  使得先前基学习器做错的训练样本在后续受到更多关注**
3. **然后基于调整后的样本分布来训练下一个基学习器**
4. **如此重复进行,  直至基学习器数目达到事先指定的值 $T$ ,  最终将这 $T$ 个基学习器进行加权结合.**



- **AdaBoosting算法**
  - 总共有T轮
  - 每轮产生就学习器h之后都要验证是否比随机猜测的误差大
    - 误差大于随机猜测（二分类就是0.5）就停止
    - 误差小于就继续下一步
  - 更新权重
  - 更新数据集
  - 循环直到有T个基学习器

![](./images/boosting.png)

- **权重更新公式**

$$
\alpha_{t}=\frac{1}{2} \ln \left(\frac{1-\epsilon_{t}}{\epsilon_{t}}\right)\tag{8.11}
$$

- **数据集更新公式**

$$
\begin{eqnarray*} \mathcal{D}_{t+1}(\boldsymbol{x}) &=&\frac{\mathcal{D}(\boldsymbol{x}) e^{-f(\boldsymbol{x}) H_{t}(\boldsymbol{x})}}{\mathbb{E}_{\boldsymbol{x} \sim \mathcal{D}}\left[e^{-f(\boldsymbol{x}) H_{t}(\boldsymbol{x})}\right]} 
\\ &=&\frac{\mathcal{D}(\boldsymbol{x}) e^{-f(\boldsymbol{x}) H_{t-1}(\boldsymbol{x})} e^{-f(\boldsymbol{x}) \alpha_{t} h_{t}(\boldsymbol{x})}}{\mathbb{E}_{\boldsymbol{x} \sim \mathcal{D}}\left[e^{-f(\boldsymbol{x}) H_{t}(\boldsymbol{x})}\right]} 
\\ &=&\mathcal{D}_{t}(\boldsymbol{x}) \cdot e^{-f(\boldsymbol{x}) \alpha_{t} h_{t}(\boldsymbol{x})} \frac{\mathbb{E}_{\boldsymbol{x} \sim \mathcal{D}}\left[e^{-f(\boldsymbol{x}) H_{t-1}(\boldsymbol{x})}\right]}{\mathbb{E}_{\boldsymbol{x} \sim \mathcal{D}}\left[e^{-f(\boldsymbol{x}) H_{t}(\boldsymbol{x})}\right]}\tag{8.19} \end{eqnarray*}
$$



- Boosting 算法要求**基学习器**能**对特定的数据分布进行学习**

  - **重赋权法(re-weighting)**

    - 在训练过程的每一轮中,  根据样本分布为每个训练样本重新赋予一个权重.

    - Boosting 算法在训练的**每一轮都要检查**当前生成的**基学习器是否满足基本条件**,  一旦条件**不满足**,  则当前基学习器即被**抛弃**,  且学习过程**停止**.

      在此种情形下,  初始设置的学习轮数 $T$ 也许遥远未达到,  可能导致最终集成中只**包含很少的基学习器而性能不佳**.

  - **重采样法(re-sampling)** 

    - 对无法接受带权样本的基学习算法,  则可通过"重采样法" (re-sampling)来处理,  即在每一轮学习中,  根据样本分布对训练集重新进行采样,  再用重采样而得的样本集对基学习器进行训练
    - 可获得"重启动"机会以避免训练过程过早停止,  即在抛弃**不满足条件**的当前基学习器之后,  可根据当前分布**重新对训练样本进行采样**,  再基于新的采样结果**重新训练出基学习器**,  从而使得学习过程可以**持续到预设的 $T$ 轮**完成.  

​	

一般而言,  这两种做法没有显著的优劣差别.  

从偏差一方差分解的角度看， **Boosting 主要关住降低偏差**，因此Boosting
能基于泛化性能相当弱的学习器构建出很强的集成.



## 3. Bagging 与随机森林

由第一节得知，如果想得到泛化能力强的学习器，个体学习期应尽可能的**相互独立**

因此我们让基学习期有**尽可能大的差异**就能保证最大可能相互独立

一种可能的做法是对训练样本进行**采样**,  **产生出若干个不同的子集**,  再从每个数据子集中训练出**一个基学习器**.  

但是如果采样出的**每个子集完全不同**,  则每个基学习器只用到了一小部分训练数据,  甚至不足以进行有效学习,  这显然**无法确保产生出比较好的基学习器**. 

- **相互交叠采集**：

随机采样出m个样本之后再把这些样本放回去重新采集，，使得下次采样时该样本仍有可能被选中，这样，经过m次随机采样操作，我们得到含m 个样本的采样集，初始训练集中有的样本在采样集里多次出现，有的则从未出现.由式(2.1)可知，初始训练集中约有63.2%的样本出现在来样集中.

- **Bagging**

我们可以**采样出 $T$ 个含 $m$ 个训练样本的采样集**,  然后基于**每个采样训练集训练出一个基学习器**,  再将这些**基学习器进行结合**. 这就是 **Bagging 的基本流程**.

在对**预测输出**进行结合时,  Bagging 通常对**分类任务**使用**简单投票法**,  对**回归任务**使用**简单平均法**. 

![](./images/bagging.png)

**训练一个 Bagging 集成与直接使用基学习算法训练一个学习器的复杂度同阶.** 这说明 Bagging 是一个很高效的集成学习算法.

另外,  与标准 **AdaBoost** 只适用于**二分类任务**不同,  **Bagging** 能不经修改地用于**多分类**、**回归等任务**. 

从偏差方差分解的角度看， **Bagging 主要关注降低方差**，因此它在不剪枝
决策树、神经网络等易受样本扰动的学习器上效用更为明显

- **包外估计**

同时,  自助采样有 36.8% 的样本可用作验证集来对泛化性能进行**"包外估计".**

包外样本还有许多其他用途, 如:

1. 当基学习器是决策树时,  可使用包外样本来辅助剪枝,  或用于估计决策树中各结点的后验概率以辅助对零训练样本结点的处理.
2. 当基学习器是是神经网络时,  可使用包外样本来辅助早期停止以减小过拟合风险. 



## 4.随机森林(Random Forest）

- **随机森林**

随机森林 (Random Forest，简称 $RF$) 是 **Bagging 的一个扩展变体**.  $RF$ 在**以决策树为基学习器**构建 Bagging 集成的基础上,  进一步在决策树的训练过程中**引入了随机属性**选择. 



- **基学习期随机属性选择**

与决策树不同，决策树是在节点选择时候选择一个最优的属性能让混乱程度降低。

**随机森林**对基决策树的每个结点,  先从**该结点的属性集合**中**随机**选择一个**包含 $k$ 个属性**的**子集**,  然后再从这个**子集中**选择一个**最优属性**用于划分. 

这里 的参数 $k$ 控制了随机性的引入程度;

若令 $k = d$,   则基决策树的构建与传统决策树相同;

 若令 $k = 1$ ,  则是随机选择一个属性用 于划分;

  一般情况下,  推荐值 $k=\log _{2}d$



- **随机森林的优异性**

随机森林简单、容易 实现、计算开销小,  但它在很多现实任务中展现出**强大的性能**,  被誉为"**代表集成学习 技术水平的方法**" 

**样本扰动和属性扰动**。这就使得最终集成的**泛化性能**可通过个体学习器之间**差异度的增加**而进一步**提升**. 

![](./images/随机森林性能.png)

## 5.结合策略

- **学习期结合的好处**
  - 假设空间很大，用单一学习器可能会导致泛化能力不强
  - 会陷入局部最小，通过多次运行进行结合，可以降低陷入局部最小的风险
  - 从表示的方面来看，某些学习任务的真实假设可能不在当前学习算法所考虑的假设空间中，此时若使用单学习器则肯定无效，而通过结合多个学习器， 由于相应的假设空间有所扩大，有可能学得更好的近似

- **平均法**

对数值型输出 $h_{i}(\boldsymbol{x}) \in \mathbb{R}$ ,  最常见的结合策略是使用平均法 (averaging).

- 简单平均法(simple averaging)
  $$
  H(\boldsymbol{x})=\frac{1}{T} \sum_{i=1}^{T} h_{i}(\boldsymbol{x})\tag{8.22}
  $$

- 加权平均法(weighted averaging)
  $$
  H(\boldsymbol{x})=\sum_{i=1}^{T} w_{i} h_{i}(\boldsymbol{x})\tag{8.23}
  $$
  其中,  $w_{i}$ 是个体学习器 $h_{i}$ 的权重, 通常要求 $w_{i} \geqslant 0, \sum_{i=1}^{T} w_{i}=1$. 

加权平均法和简单平均法之间的关系:

简单平均法是加权平均法令 $w_{i}=1 / T$  的特例. 

事实上,  加权平均法可认为是集成学习研究的**基本出发点.**

加权平均法的**权重**一般是**从训练数据中学习**而得,  现实任务中的训练样本通常不充分或存在噪声,  这将使得学出的权重**不完全可靠**.  当集成规模较大时,  学习的权重比较多,  容易导致过拟合.

**加权平均法未必一起优于简单平均法**,  

**选择的原则:**

一般而言,  在个体**学习器性能相差较大**时宜使用**加权平均法**,  而在个体**学习器性能相近**时宜使用**简单平均法**.  



- **投票法**

对分类任务来说,  学习器 $h_{i}$ 将从类别标记集合 $\left\{c_{1}, c_{2}, \ldots, c_{N}\right\}$ 中预测出一个标记,  最常见的结合策略是使用**投票法** (voting) . 

将 $h_{i}$ 在样本 $\boldsymbol x$ 上的预测输出表示为一个 $N$ 维向量 $\left(h_{i}^{1}(\boldsymbol{x}) ; h_{i}^{2}(\boldsymbol{x}) ; \ldots ; h_{i}^{N}(\boldsymbol{x})\right)$ ,  其中 $h_{i}^{j}(\boldsymbol x)$ 是 $h_{i}$ 在类别标记 $c_{j}$ 上的输出.

- **绝对多数投票法**(majority voting)

$$
H(\boldsymbol{x})=\left\{\begin{array}{ll}{c_{j},} & {\text { if } \sum_{i=1}^{T} h_{i}^{j}(\boldsymbol{x})>0.5 \sum_{k=1}^{N} \sum_{i=1}^{T} h_{i}^{k}(\boldsymbol{x})} \\ {\text { reject, }} & {\text { otherwise. }}\end{array}\right.\tag{8.24}
$$

​        **即若某标记得票过半数,  则预测为该标记**;  否则拒绝预测.

- **相对多数投票法**(plurality voting)
  $$
  H(\boldsymbol{x})=c_{\underset{j}{\arg \max } \sum_{i=1}^{T} h_{i}^{j}(\boldsymbol{x})}\tag{8.25}
  $$

​       **即预测为的票数最多的标记,**  若同时有多个标记获得最高票,  则从中随机选取一个.

- **加权投票法**(weighted voting)
  $$
  H(\boldsymbol{x})=c_{\arg \max } \sum_{i=1}^{T} w_{i} h_{i}^{j}(\boldsymbol{x})\tag{8.26}
  $$
  与加权平均法类似,  $w_{i}$ 是 $h_{i}$ 的权重, 通常要求 $w_{i} \geqslant 0, \sum_{i=1}^{T} w_{i}=1$. 

需注意的是，若基学习器的类型不同?则
其类概率值不能直接进行比较;在此种情形下，通常可将类概率输出转化为类
标记输出(例如将类概率输出最大的时(x) 设为1 ，其他设为0) 然后再投票



- **学习法**

当训练数据很多时,  一种更为强大的结合策略是使用"学习法",  即通过另一个学习器来进行结合. Stacking 是学习法的典型代表.

在这个新数据集中,  **初级学习器的输出被当作样例输入特征**,  **而初始样本的标记仍被当作样例标记**. Stacking 的算法描述如图 8.9 所示. 同时,  初级集成是异质的.



- **多样性**

**适用回归学：个体学习器准确性越高、多样性越大，则集成越好**

- **多样性度量**

**适用分类：增加多样性：样本扰动，属性扰动，算法扰动，输出扰动**





# python代码

# sklearn

## 7.决策树

```python
sklearn.tree.DecisionTreeClassifier
(*, criterion='gini', splitter='best', max_depth=None, min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features=None, random_state=None, max_leaf_nodes=None, min_impurity_decrease=0.0, min_impurity_split=None, class_weight=None, ccp_alpha=0.0
```

- **参数**
  - **criterion: {“gini”, “entropy”}, default=”gini”**
    - 算法，用来分支的计算方法
    - ”gini” 默认是基尼系数，表示在这个属性下能选出不同类的概率
    - “entropy” 信息熵，混乱程度
  - **splitter ; {“best”, “random”}, default=”best”**
    - 特征划分点选择不熬转
    - “best” 寻找最优划分
    - “random” 随机划分
  - **max_depth ：int, default=None**
    - 决策树的最大深度
  - **min_samples_split ; int or float, default=2**
    - 节点再划分时候所需的最小样本数
    - 默认是2，如果传入int则整数作为最小样本数
    - 如果为float，`min_samples_split`则为分数意思为百分比，用该份数乘以样本数的结果作为最小样本数
  - **min_samples_leaf ： int or float, default=1**
    - 在叶节点处需要的最小样本数
    - 如果为int，则认为`min_samples_leaf`是最小值。
    - 如果为float，`min_samples_leaf`则为分数， 是每个节点的最小样本数。`ceil(min_samples_leaf * n_samples)`
  - **min_weight_fraction_leaf：：float, default=0.0**
    - 在所有叶节点处（所有输入样本）的权重总和中的最小加权分数。如果未提供sample_weight，则样本的权重相等。
  - **max_features： int, float or {“auto”, “sqrt”, “log2”}, default=None**
    - 划分时考虑的最大特征数
    - 默认是"None"，表示分时考虑所有的特征数
    - 如果是整数，代表考虑的特征绝对数
    - 如果是浮点数，代表考虑特征百分比，即考虑（百分比xN）取整后的特征数。其中N为样本总特征数。
    - "log2"意味着划分时最多考虑log2N个特征
    - sqrt"或者"auto"意味着划分时最多考虑根号下N个特征。
  - **random_state ： int, RandomState instance or None, default=None**
    - 算法将在每个拆分中随机选择，然后找到它们之间的最佳拆分。但是最佳发现的拆分可能会因不同的运行而有所不同
    - 如果是int，则random_state是随机数生成器使用的种子; 
    - 如果是RandomState实例，则random_state是随机数生成器; 
    - 如果为None，则随机数生成器是np.random使用的RandomState实例。
  - **max_leaf_nodes： int, default=None**
    - 通过限制最大叶子节点数，可以防止过拟合，默认是"None”，即不限制最大的叶子节点数。
  - **min_impurity_decrease ：float, default=0.0**
    - 如果节点分裂会导致混乱的减少大于或等于该值，则该节点将被分裂
  - **min_impurity_split ：float, default=0**
    - 节点划分最小不纯度，这个值限制了决策树的增长，如果某节点的不纯度(基尼系数，信息增益，均方差，绝对差)小于这个阈值，则该节点不再生成子节点
  - **class_weight*****dict, list of dict or “balanced”, default=None**
    - 与形式的类有关的权重。如果为None，则所有类的权重都应为1。对于多输出问题，可以按与y列相同的顺序提供字典列表
      - 注意，应当为onehot编码的编号1处都定义权重，比如四分类 [{0: 1, 1: 1}, {0: 1, 1: 5}, {0: 1, 1: 1}, {0: 1, 1: 1}] 而不是 [{1:1}, {2:5}, {3:1}, {4:1}].
    - 如果使用“balanced”，则算法会自己计算权重，样本量少的类别所对应的样本权重会高。默认是None
    - 如果指定了sample_weight，则这些权重将与sample_weight（通过fit方法传递）相乘
  - **ccp_alpha ; non-negative float, default=0.0**
    - 最小剪枝系数，默认为0
    - 该参数用来限制树过拟合的剪枝参数,，模型将会选择小于输入值最大α ,ccp_alpha=0时，决策树不剪枝；ccp_alpha越大，越多的节点被剪枝。Scikit learn 0.22版本新增参数。

- **属性**
  - **classes_** ： 形状（n_classes）的ndarray或ndarray的列表**
    - 类标签
  - **feature_importances_ ： ndarray of shape (n_features,)**
    - 特征重要性。该值越高，该特征越重要。
    - 特征的重要性为该特征导致的评价准则的（标准化的）总减少量。它也被称为基尼的重要性
  - **max_features_： int**
    - 最大迭代次数
  - **n_classes_ ： int or list of int**
    - 类的数量
  - **n_features_ ：int**
    - 执行fit的时候，特征的数量。
  - **tree_： Tree instance**
    - 底层的Tree对象。
- **方法**
  - **apply（X）**
    - 返回每个样本被预测的叶子的索引
  - **cost_complexity_pruning_path（X，y，sample_weight=None)**
    - 计算修剪路径
  - **decision_path（X，check_input=True)**
    - 返回决策路径，是个稀疏矩阵
  - **fit(*X*, *y*, *sample_weight=None*)**
    - 训练分类器模型
  - **fit_predict(X,y)**
    - 训练模拟器并预测每个样本的聚类
  - **get_depth()**
    - 返回决策树的深度
  - **get_n_leaves()**
    - 返回决策树的叶子数
  - **get_params(deep=True)**
    - deep ： bool 默认为True
    - 返回字典，估计器的各个设置参数
  - **predict（X）**
    - 用估计其预测X，得到预测值
  - **predict_log_proba（X）**
    - 预测X的类对数概率
  - **predict_proba（X）**
    - 预测X的概率
  - **score(X,y,sample_weight)：**
    - 返回（X，y）上的的平均准确度
  - **set_params()**
    - 该估计其的设置

- **备注**

控制树（例如`max_depth`，`min_samples_leaf`等）大小的参数的默认值会导致树完全生长和未修剪，这在某些数据集上可能非常大。为了减少内存消耗，应通过设置这些参数值来控制树的复杂性和大小。

- class sklearn.tree.DecisionTreeRegressor**回归决策树，参数和分类决策树相似，是用决策树的方法来做回归问题拟合曲线。
- **可视化**

```python
# 先安装python-graphviz  用下面的方法会生成一个图文件和一个pdf
# 可视化
clf = tree.DecisionTreeClassifier(criterion='entropy')

# 生成一个pdf
import graphviz
dot_data = tree.export_graphviz(clf, out_file=None)
graph = graphviz.Source(dot_data)
graph.render("iris")
```

## 8.集成学习

### 8.1 adaboost 分类和回归

```python
sklearn.ensemble.AdaBoostClassifier
(base_estimator=None, *, n_estimators=50, learning_rate=1.0, algorithm='SAMME.R', random_state=None
```

- **参数**
  - **base_estimator ： object, default=None**
    - 集成学习基本估计器，如果为None则会自动选择决策树来估计
  - **n_estimators ： int, default=50**
    - 集成学习估计器的数量
  - **learning_rate ： float, default=1.**
    - 学习率，缩小了每个估计器的贡献
  - **algorithm ： {‘SAMME’, ‘SAMME.R’}, default=’SAMME.R’**
    - 如果为“ SAMME.R”，则使用SAMME.R实际增强算法`base_estimator`必须支持类概率的计算，收敛更快
    - 如果为“ SAMME”，则使用SAMME离散提升算法。
  - **random_state ： int, RandomState instance or None, default=None**
    - 控制`base_estimator`在每次增强迭代中每个给定的随机种子
    - 如果是int，则random_state是随机数生成器使用的种子; 
    - 如果是RandomState实例，则random_state是随机数生成器; 
    - 如果为None，则随机数生成器是np.random使用的RandomState实例。
- **属性**
  - **base_estimator_ ：estimator**
    - 最好的估计器
  - **estimators_: list of classifiers**
    - 估计器的集合
  - **classes_ ： ndarray of shape (n_classes,)**
    - 类的标签
  - **n_classes_   ：int***
    - 类数量
  - **estimator_weights_ : ndarray of floats***
    - 每个估计器的权重
  - **estimator_errors_ ： ndarray of floats**
    - 每个估计器的分类误差
  - **feature_importances_  :ndarray of shape (n_features,)**
    - 特征的重要性，请参阅文档，不明所以

- **方法**
  - **decision_function（X）**
    - 在最佳估计器上调用预测样本的置信度概率
  - **fit(*X*, *y*, *sample_weight=None*)**
    - 训练
  - **get_params(deep=True)**
    - deep ： bool 默认为True
    - 返回字典，估计器的各个设置参数
  - **predict（X）**
    - 用最佳的估计其预测X，得到预测值
  - **predict_log_proba（*X*）**
    - 用最佳估计器，返回一个数组，数组的元素一次是 X 预测为各个类别的概率的对数值。
  - **predict_proba（X）：**
    - 用最佳估计器，返回一个数组，数组元素一次是 X 预测为各个类别的概率的概率值。
  - **score(X,y,sample_weight)：**
    - 用最佳估计器，返回（X，y）上的预测准确率（accuracy）。
  - **set_params()**
    - 该估计其的设置
  - **staged_decision_function（X）**
    - 每次迭代的X的置信区间概率
  - **staged_predict（X）**
    - 返回每个基分类器的预测数据集X的结果。
  - **staged_predict_proba(X)**:
    - 返回每个基分类器的预测数据集X的概率结果
  - **staged_score(X, Y)**
    - 返回每个基分类器的预测准确率。





**adaboost回归**

```python
sklearn.ensemble.AdaBoostRegressor(base_estimator=None, *, n_estimators=50, learning_rate=1.0, loss='linear', random_state=None)
```



- 和分类的参数相似只不过算法为loss函数，方法略有改变

- **loss** *{'linear'，'square'，'exponential'}，默认=“ linear”*

  每次增强迭代后更新权重时使用的损失函数。



### 8.2  bagging 分类和回归

**bagging分类**

```python
sklearn.ensemble.BaggingClassifier
(base_estimator=None, n_estimators=10, *, max_samples=1.0, max_features=1.0, bootstrap=True, bootstrap_features=False, oob_score=False, warm_start=False, n_jobs=None, random_state=None, verbose=0)
```

- **参数**
  - **base_estimator ： object, default=None**
    - 集成学习基本估计器，如果为None则会自动选择决策树来估计
  - **n_estimators ： int, default=50**
    - 集成学习估计器的数量
  - **max_samples ： int or float, default=1.0**
    - 从从X抽取以训练每个基本估计量的样本数
      - 如果为int，则抽取`max_samples`样本。
      - 如果漂浮，则抽取样品。`max_samples * X.shape[0]`
  - **max_features ： int or float, default=1.0**
    - 决定从x_train抽取去训练基估计器的特征数量。
    - 如果为int，则绘制`max_features`特征。
    - 如果是浮点的，则为比例。`max_features * X.shape[1]`
  - **bootstrap ： bool, default=True**
    - 决定样本子集的抽样方式（有放回和不放回）
  - **bootstrap_features ： bool, default=False**
    - 决定特征子集的抽样方式（有放回和不放回）
  - **oob_score： bool, default=False**
    - 决定是否使用包外估计（out of bag estimate）泛化误差
  - **warm_start ： bool, default=False**
    - 是否热启动，为True时会从接着上一次计算开始继续计算
  - **n_jobs ：int, default=None**
    - 并行计算的cpu数量，-1表示用所有的cpu
  - **verbose ; int, default=0**
    - 在拟合和预测时控制详细程度。
  - **random_state ： int, RandomState instance or None, default=None**
    - 控制`base_estimator`在每次增强迭代中每个给定的随机种子
    - 如果是int，则random_state是随机数生成器使用的种子; 
    - 如果是RandomState实例，则random_state是随机数生成器; 
    - 如果为None，则随机数生成器是np.random使用的RandomState实例。

- **属性**
  - **base_estimator_： estimator**
    - 基估计器的种类
  - **n_features_ ：int**
    - 拟合的特征数量
  - **estimators_** : list of estimators
    - 基本估计器的列表
  - **estimators_samples_** : list of arrays
    - 每个基本估计量的抽取样本的子集。
  - **estimators_features_** : list of arrays
    - 每个基本估计量的特征子集。
  - **classes_： ndarray of shape (n_classes,)**
    - 类的标签
  - **n_classes_   ：int**
    - 类数量
  - **oob_score_** : float，
    - 使用包外估计这个训练数据集的得分。
  - **oob_prediction_** : array of shape = [n_samples]。
    - 在训练集上用out-of-bag估计计算的预测。 如果n_estimator很小，则可能在抽样过程中数据点不会被忽略。 在这种情况下，oob_prediction_可能包含NaN。
- **方法**
  - **decision_function（X）**
    - 在最佳估计器上调用预测样本的置信度概率
  - **fit(*X*, *y*, *sample_weight=None*)**
    - 训练
  - **get_params(deep=True)**
    - deep ： bool 默认为True
    - 返回字典，估计器的各个设置参数
  - **predict（X）**
    - 用最佳的估计其预测X，得到预测值
  - **predict_log_proba（*X*）**
    - 用最佳估计器，返回一个数组，数组的元素一次是 X 预测为各个类别的概率的对数值。
  - **predict_proba（X）：**
    - 用最佳估计器，返回一个数组，数组元素一次是 X 预测为各个类别的概率的概率值。
  - **score(X,y,sample_weight)：**
    - 用最佳估计器，返回（X，y）上的预测准确率（accuracy）。
  - **set_params()**
    - 该估计其的设置





**bagging回归**

```python
sklearn.ensemble.BaggingRegressor
(base_estimator=None, n_estimators=10, *, max_samples=1.0, max_features=1.0, bootstrap=True, bootstrap_features=False, oob_score=False, warm_start=False, n_jobs=None, random_state=None, verbose=0)
```

- 参数属性方法和分类差不多



### 8.3 随机森林RandomForest

**随机森林分类器**

```python
sklearn.ensemble.RandomForestClassifier
(n_estimators=100, *, criterion='gini', max_depth=None, min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features='auto', max_leaf_nodes=None, min_impurity_decrease=0.0, min_impurity_split=None, bootstrap=True, oob_score=False, n_jobs=None, random_state=None, verbose=0, warm_start=False, class_weight=None, ccp_alpha=0.0, max_samples=None）
```

- 参数
  - **n_estimators： int, default=100**
    - 森林里的树木数量120,200,300,500,800,1200
      - 在利用最大投票数或平均值来预测之前，你想要建立子树的数量。
  - **criterion: {“gini”, “entropy”}, default=”gini”**
    - 算法，用来分支的计算方法
    - ”gini” 默认是基尼系数，表示在这个属性下能选出不同类的概率
    - “entropy” 信息熵，混乱程度
  - **max_depth ：int, default=None**
    - 决策树的最大深度
  - **min_samples_split ; int or float, default=2**
    - 节点再划分时候所需的最小样本数
    - 默认是2，如果传入int则整数作为最小样本数
    - 如果为float，`min_samples_split`则为分数意思为百分比，用该份数乘以样本数的结果作为最小样本数
  - **min_samples_leaf ： int or float, default=1**
    - 在叶节点处需要的最小样本数
    - 如果为int，则认为`min_samples_leaf`是最小值。
    - 如果为float，`min_samples_leaf`则为分数， 是每个节点的最小样本数。`ceil(min_samples_leaf * n_samples)`
  - **min_weight_fraction_leaf：：float, default=0.0**
    - 在所有叶节点处（所有输入样本）的权重总和中的最小加权分数。如果未提供sample_weight，则样本的权重相等。
  - **max_features： int, float or {“auto”, “sqrt”, “log2”}, default=None**
    - 划分时考虑的最大特征数
    - 默认是"None"，表示分时考虑所有的特征数
    - 如果是整数，代表考虑的特征绝对数
    - 如果是浮点数，代表考虑特征百分比，即考虑（百分比xN）取整后的特征数。其中N为样本总特征数。
    - "log2"意味着划分时最多考虑log2N个特征
    - sqrt"或者"auto"意味着划分时最多考虑根号下N个特征。
  - **max_leaf_nodes： int, default=None**
    - 通过限制最大叶子节点数，可以防止过拟合，默认是"None”，即不限制最大的叶子节点数。
  - **min_impurity_decrease ：float, default=0.0**
    - 如果节点分裂会导致混乱的减少大于或等于该值，则该节点将被分裂
  - **min_impurity_split ：float, default=None**
    - 节点划分最小不纯度，这个值限制了决策树的增长，如果某节点的不纯度(基尼系数，信息增益，均方差，绝对差)小于这个阈值，则该节点不再生成子节点
  - **bootstrap： bool, default=True**
    - 是否在构建树时使用放回抽样
  - **oob_score： bool, default=False**
    - 是否使用包外估计来泛化精度
  - **n_jobs ： int, default=None**
    - 并行计算cpu使用数，-1表示使用所有cpu
  - **random_state ：int, RandomState instance or None, default=None**
    - 如果是int，则random_state是随机数生成器使用的种子; 
    - 如果是RandomState实例，则random_state是随机数生成器; 
    - 如果为None，则随机数生成器是np.random使用的RandomState实例。
  - **verbose int, default=0**
    - 详细程度
  - **warm_start ： bool, default=False**
    - 是否热启动，为True时会从接着上一次计算开始继续计算
  - **class_weight ： {“balanced”, “balanced_subsample”}, dict or list of dicts, default=None**
    - 与形式的类有关的权重。如果为None，则所有类的权重都应为1。对于多输出问题，可以按与y列相同的顺序提供字典列表
      - 注意，应当为onehot编码的编号1处都定义权重，比如四分类 [{0: 1, 1: 1}, {0: 1, 1: 5}, {0: 1, 1: 1}, {0: 1, 1: 1}] 而不是 [{1:1}, {2:5}, {3:1}, {4:1}].
    - 如果使用“balanced”，则算法会自己计算权重，样本量少的类别所对应的样本权重会高。模式使用y值自动调整与输入数据中的类频率成反比的权重，如下所示： `n_samples / (n_classes * np.bincount(y))`
    - “ balanced_subsample”模式与“ balanced”相同，不同之处在于，权重是根据每个树生长的引导程序样本计算的。
    - 如果指定了sample_weight，则这些权重将与sample_weight（通过fit方法传递）相乘
  - **ccp_alpha ; non-negative float, default=0.0**
    - 最小剪枝系数，默认为0
    - 该参数用来限制树过拟合的剪枝参数,，模型将会选择小于输入值最大α ,ccp_alpha=0时，决策树不剪枝；ccp_alpha越大，越多的节点被剪枝。Scikit learn 0.22版本新增参数。
  - **max_samples ： int or float, default=None**
    - 如果bootstrap为True，则从X抽取以训练每个基本估计量的样本数。
    - 如果为“无”（默认），则绘制`X.shape[0]`样本。
    - 如果为int，则抽取`max_samples`样本。
    - 如果漂浮，则抽取样品。因此， 应在间隔内。`max_samples * X.shape[0]``max_samples``(0, 1)`

- **属性**
  - **base_estimator_： estimator**
    - 基估计器的种类
  - **n_features_ ：int**
    - 拟合的特征数量
  - **n_outputs_ ： int**
    - 输出的数量
  - **estimators_** : list of estimators
    - 基本估计器的列表
  - **classes_： ndarray of shape (n_classes,)**
    - 类的标签
  - **n_classes_   ：int**
    - 类数量
  - **oob_score_** : float，
    - 使用包外估计这个训练数据集的得分。
  - **oob_prediction_** : array of shape = [n_samples]。
    - 在训练集上用out-of-bag估计计算的预测。 如果n_estimator很小，则可能在抽样过程中数据点不会被忽略。 在这种情况下，oob_prediction_可能包含NaN。
- **方法**
  - **apply（X）**
    - 返回每个样本被预测的叶子的索引
  - **decision_path（X，check_input=True)**
    - 返回决策路径，是个稀疏矩阵
  - **fit(*X*, *y*, *sample_weight=None*)**
    - 训练分类器模型
  - **get_params(deep=True)**
    - deep ： bool 默认为True
    - 返回字典，估计器的各个设置参数
  - **predict（X）**
    - 用估计其预测X，得到预测值
  - **predict_log_proba（X）**
    - 预测X的类对数概率
  - **predict_proba（X）**
    - 预测X的概率
  - **score(X,y,sample_weight)：**
    - 返回（X，y）上的的平均准确度
  - **set_params()**
    - 该估计其的设置



**随机森林回归**

```python
sklearn.ensemble.RandomForestRegressor
(n_estimators=100, *, criterion='mse', max_depth=None, min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features='auto', max_leaf_nodes=None, min_impurity_decrease=0.0, min_impurity_split=None, bootstrap=True, oob_score=False, n_jobs=None, random_state=None, verbose=0, warm_start=False, ccp_alpha=0.0, max_samples=None)
```

- 和分类类似，只不过算法不用基尼系数
- **criterion*****{“mse”, “mae”}, default=”mse”***
  - 衡量分割质量的功能。支持的标准是均方误差的“ mse”（等于特征选择标准的方差减少）和均值绝对误差的“ mae”。

